{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dex9EtVHlYPP",
        "outputId": "47d15993-bf96-4511-e93f-89db807348bc"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '\"c:/Users/HP ProBook A9/AppData/Local/Microsoft/WindowsApps/python3.11.exe\" -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install --upgrade pip\n",
        "\n",
        "# Current stable release for CPU and GPU\n",
        "!pip install tensorflow\n",
        "\n",
        "# Or try the preview build (unstable)\n",
        "!pip install tf-nightly\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFJbmWT6XbgY",
        "outputId": "af36f922-3d8d-4d2e-994c-b3e99b99c383"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m721.6/721.6 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.7/73.7 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.9/489.9 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.7/33.7 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for jax (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sqlalchemy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albucore 0.0.13 requires typing-extensions>=4.9.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "albumentations 1.4.14 requires scipy>=1.10.0, but you have scipy 1.9.3 which is incompatible.\n",
            "albumentations 1.4.14 requires typing-extensions>=4.9.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "chex 0.1.86 requires jax>=0.4.16, but you have jax 0.4.14 which is incompatible.\n",
            "flax 0.8.4 requires jax>=0.4.19, but you have jax 0.4.14 which is incompatible.\n",
            "google-colab 1.0.0 requires portpicker==1.5.2, but you have portpicker 1.6.0 which is incompatible.\n",
            "ipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.4.20 which is incompatible.\n",
            "orbax-checkpoint 0.6.1 requires jax>=0.4.26, but you have jax 0.4.14 which is incompatible.\n",
            "pandas-stubs 2.1.4.231227 requires numpy>=1.26.0; python_version < \"3.13\", but you have numpy 1.25.2 which is incompatible.\n",
            "pydantic 2.8.2 requires typing-extensions>=4.6.1; python_version < \"3.13\", but you have typing-extensions 4.5.0 which is incompatible.\n",
            "pydantic-core 2.20.1 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 4.25.4 which is incompatible.\n",
            "tensorstore 0.1.64 requires ml-dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.14.1 which is incompatible.\n",
            "tf-nightly 2.18.0.dev20240828 requires ml-dtypes<0.5.0,>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\n",
            "torch 2.4.0+cu121 requires typing-extensions>=4.8.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "typeguard 4.3.0 requires typing-extensions>=4.10.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "xarray 2024.6.0 requires packaging>=23.1, but you have packaging 22.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: ml_dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Requirement already satisfied: numpy>1.20 in /usr/local/lib/python3.10/dist-packages (from ml_dtypes==0.2.0) (1.25.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install --quiet tensorflow-federated\n",
        "!pip install --quiet --upgrade tensorflow-federated\n",
        "!pip install --quiet --upgrade nest-asyncio\n",
        "!pip install --upgrade ml_dtypes==0.2.0\n",
        "# Requires the latest pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXxTzaqvmWX9"
      },
      "outputs": [],
      "source": [
        "#import tensorflow as tf\n",
        "#print(\"TensorFlow version:\", tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MfxeYgymmaZ"
      },
      "outputs": [],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6z95EoyKi0Vx",
        "outputId": "3c23d7c9-8bec-4160-fb3e-e37922ffc25a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.4\n",
            "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
          ]
        }
      ],
      "source": [
        "# Mounts Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import collections\n",
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras\n",
        "from keras.layers import Dense\n",
        "import matplotlib.pyplot as plt\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LmK1SzTMvJl3"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84eOjHQNju77",
        "outputId": "44335096-e0bd-485c-8092-47a0241a4628"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<tensorflow_federated.python.core.impl.computation.computation_impl.ConcreteComputation object at 0x7a242e734580>\n"
          ]
        }
      ],
      "source": [
        "import tensorflow_federated as tff\n",
        "print(tff.federated_computation(lambda: 'Hello World'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AV8hGEHQKYNE"
      },
      "source": [
        "# **Wustl-ehms-2020 Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSpxGkHm5Yvd"
      },
      "outputs": [],
      "source": [
        "df_wus = pd.read_csv('/content/drive/MyDrive/GlobeCom/clean_wustl.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0IE9naY5g8P"
      },
      "outputs": [],
      "source": [
        "df_wus.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6IF7xN9pzDg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "for i, df in enumerate(np.array_split(df_wus, 15)):\n",
        "    df.to_csv('/content/drive/MyDrive/15-clients/'+f\"wus{i+1}.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8s65rkSl5Ql5"
      },
      "outputs": [],
      "source": [
        "# Final features\n",
        "feature_columns = [\"Time\", \"Source\", \"Destination\", \"Protocol\", \"Length\"]\n",
        "df_treated = client1_ECU[feature_columns + [\"Type\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0d5m6Xhq8DY"
      },
      "outputs": [],
      "source": [
        "df_wus1 = pd.read_csv('/content/drive/MyDrive/Ten_clients/wus1.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-mNjP2M6XWY"
      },
      "outputs": [],
      "source": [
        "df_wus1.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_g5I_vOfCYq"
      },
      "source": [
        "# ***Fifteen Clients***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8wZyx_sfRae"
      },
      "outputs": [],
      "source": [
        "client1 = pd.read_csv('/content/drive/MyDrive/15-clients/wus1.csv')\n",
        "client1_shuffled = client1.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "client2 = pd.read_csv('/content/drive/MyDrive/15-clients/wus2.csv')\n",
        "client2_shuffled = client2.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "client3 = pd.read_csv('/content/drive/MyDrive/15-clients/wus3.csv')\n",
        "client3_shuffled = client3.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "client4 = pd.read_csv('/content/drive/MyDrive/15-clients/wus4.csv')\n",
        "client4_shuffled = client4.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "client5 = pd.read_csv('/content/drive/MyDrive/15-clients/wus5.csv')\n",
        "client5_shuffled = client5.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "client6 = pd.read_csv('/content/drive/MyDrive/15-clients/wus6.csv')\n",
        "client6_shuffled = client6.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "client7 = pd.read_csv('/content/drive/MyDrive/15-clients/wus7.csv')\n",
        "client7_shuffled = client7.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "client8 = pd.read_csv('/content/drive/MyDrive/15-clients/wus8.csv')\n",
        "client8_shuffled = client8.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "client9 = pd.read_csv('/content/drive/MyDrive/15-clients/wus9.csv')\n",
        "client9_shuffled = client9.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "client10 = pd.read_csv('/content/drive/MyDrive/15-clients/wus10.csv')\n",
        "client10_shuffled = client10.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "client11 = pd.read_csv('/content/drive/MyDrive/15-clients/wus11.csv')\n",
        "client11_shuffled = client11.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "client12 = pd.read_csv('/content/drive/MyDrive/15-clients/wus12.csv')\n",
        "client12_shuffled = client12.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "client13 = pd.read_csv('/content/drive/MyDrive/15-clients/wus13.csv')\n",
        "client13_shuffled = client13.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "client14 = pd.read_csv('/content/drive/MyDrive/15-clients/wus14.csv')\n",
        "client14_shuffled = client14.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "client15 = pd.read_csv('/content/drive/MyDrive/15-clients/wus15.csv')\n",
        "client15_shuffled = client15.sample(frac=1, random_state = 13).reset_index(drop = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OiI8cqTeidXz"
      },
      "outputs": [],
      "source": [
        "client1_shuffled.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5MhO8v7jlOg"
      },
      "outputs": [],
      "source": [
        "client1_shuffled.Label.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ht9RjJ-NfV3f"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 100\n",
        "BATCH_SIZE = 1024\n",
        "#BATCH_SIZE = 64 #ECU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XonVSDvcfY-h"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def make_tf_dataset(dataframe, negative_ratio=None, batch_size=None):\n",
        "\n",
        "    dataset = dataframe.drop(['Unnamed: 0'], axis=1)#, 'SrcGap', 'DstGap', 'DIntPktAct','sMinPktSz', 'Trans'\n",
        "    # Class balancing\n",
        "    pos_df = dataset[dataset['Label'] == 1]\n",
        "    neg_df = dataset[dataset['Label'] == 0]\n",
        "    if negative_ratio:\n",
        "        neg_df = neg_df.iloc[random.sample(range(0, len(neg_df)), len(pos_df)*negative_ratio), :]\n",
        "    balanced_df = pd.concat([pos_df, neg_df], ignore_index=True, sort=False)\n",
        "\n",
        "    y = balanced_df.pop('Label')\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((balanced_df.values, y.to_frame().values))\n",
        "    dataset = dataset.shuffle(4048, seed=SEED) #2048\n",
        "    if batch_size:\n",
        "        dataset = dataset.batch(batch_size)\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mf8z0EhifkNT"
      },
      "outputs": [],
      "source": [
        "SEED= 1337"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fAQFgC6fg5Y"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "train_data, val_data = [], []\n",
        "\n",
        "for client_data in [client1_shuffled, client2_shuffled, client3_shuffled, client4_shuffled, client5_shuffled,\n",
        "                    client6_shuffled, client7_shuffled, client8_shuffled, client9_shuffled, client10_shuffled,\n",
        "                    client11_shuffled, client12_shuffled, client13_shuffled, client14_shuffled, client15_shuffled]:\n",
        "\n",
        "\n",
        "    train_df, val_df = train_test_split(client_data, test_size=0.1, random_state=1337)\n",
        "\n",
        "    # Scaling (Standardization actually hurts performance)\n",
        "    encoder = LabelEncoder()\n",
        "    scaler = StandardScaler()\n",
        "    train_features = scaler.fit_transform(train_df.drop(['Label'], axis=1))\n",
        "    val_features = scaler.transform(val_df.drop(['Label'], axis=1))\n",
        "\n",
        "    train_df[train_df.columns.difference(['Label'])] = train_features\n",
        "    val_df[val_df.columns.difference(['Label'])] = val_features\n",
        "\n",
        "    # TF Datasets\n",
        "    train_data.append(make_tf_dataset(train_df,negative_ratio=2, batch_size=BATCH_SIZE)) #negative_ratio=2\n",
        "    val_data.append(make_tf_dataset(val_df, batch_size=16))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dq1i0hbfo_O"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.metrics import CategoricalAccuracy, Precision, Recall, BinaryAccuracy\n",
        "def input_spec():\n",
        "    return (\n",
        "        tf.TensorSpec([None, 35], tf.float64),\n",
        "        tf.TensorSpec([None, 1], tf.int64)\n",
        "    )\n",
        "\n",
        "def model_fn():\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.InputLayer(input_shape=(35,)),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dense(32, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid'),\n",
        "    ])\n",
        "\n",
        "    return tff.learning.from_keras_model(\n",
        "        model,\n",
        "        input_spec=input_spec(),\n",
        "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "        metrics=[tf.keras.metrics.BinaryAccuracy(), Precision(), Recall(), AUC()]\n",
        "        #tfa.metrics.F1Score(num_classes=2, threshold=0.5)\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XaYWzOEAfvrm"
      },
      "outputs": [],
      "source": [
        "trainer = tff.learning.build_federated_averaging_process(\n",
        "    model_fn,\n",
        "    client_optimizer_fn=lambda: tf.keras.optimizers.Adam(learning_rate=0.01),\n",
        "    server_optimizer_fn=lambda: tf.keras.optimizers.Adam(learning_rate=0.05)#learning_rate=0.01\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwW4s8J8gBTH"
      },
      "outputs": [],
      "source": [
        "str(trainer.initialize.type_signature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1bsKkkbngFE-"
      },
      "outputs": [],
      "source": [
        "state = trainer.initialize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INjx_O96f-lP"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "start = time.time()\n",
        "start_time = time.time()\n",
        "end = time.time()\n",
        "diff=end-start\n",
        "starttest = time.time()\n",
        "endtest =time.time()\n",
        "difftest = endtest-starttest\n",
        "#state = trainer.initialize()\n",
        "train_hist = []\n",
        "for i in range(EPOCHS):\n",
        "    state, metrics = trainer.next(state, train_data)\n",
        "    train_hist.append(metrics)\n",
        "\n",
        "    print(f\"\\rRun {i+1}/{EPOCHS}\", end=\"\")\n",
        "endtest =time.time()\n",
        "#difftest = endtest-starttest\n",
        "print(\"\\nTraining time: \" + str(diff))\n",
        "print(\"\\nTest time: \" + str(difftest))\n",
        "time_required = time.time() - start_time\n",
        "print('\\nTIME: {} seconds'.format(time_required))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_F8CfQ3LkasV"
      },
      "outputs": [],
      "source": [
        "evaluator = tff.learning.build_federated_evaluation(model_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gNJBMJ47kc5g"
      },
      "outputs": [],
      "source": [
        "#100 epoch\n",
        "federated_metrics = evaluator(state.model, val_data)\n",
        "federated_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aaE49qS0kfHZ"
      },
      "outputs": [],
      "source": [
        "print(metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VesOIo3Hkg1U"
      },
      "outputs": [],
      "source": [
        "train_hist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zU0981edis43"
      },
      "source": [
        "# **Ten Clients**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yHqmrDSs48J"
      },
      "outputs": [],
      "source": [
        "client1 = pd.read_csv('/content/drive/MyDrive/Ten_clients/wus1.csv')\n",
        "client1_shuffled = client1.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "client2 = pd.read_csv('/content/drive/MyDrive/Ten_clients/wus2.csv')\n",
        "client2_shuffled = client2.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "client3 = pd.read_csv('/content/drive/MyDrive/Ten_clients/wus3.csv')\n",
        "client3_shuffled = client3.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "client4 = pd.read_csv('/content/drive/MyDrive/Ten_clients/wus4.csv')\n",
        "client4_shuffled = client4.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "client5 = pd.read_csv('/content/drive/MyDrive/Ten_clients/wus5.csv')\n",
        "client5_shuffled = client5.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "client6 = pd.read_csv('/content/drive/MyDrive/Ten_clients/wus6.csv')\n",
        "client6_shuffled = client6.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "client7 = pd.read_csv('/content/drive/MyDrive/Ten_clients/wus7.csv')\n",
        "client7_shuffled = client7.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "client8 = pd.read_csv('/content/drive/MyDrive/Ten_clients/wus8.csv')\n",
        "client8_shuffled = client8.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "client9 = pd.read_csv('/content/drive/MyDrive/Ten_clients/wus9.csv')\n",
        "client9_shuffled = client9.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "client10 = pd.read_csv('/content/drive/MyDrive/Ten_clients/wus10.csv')\n",
        "client10_shuffled = client10.sample(frac=1, random_state = 13).reset_index(drop = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F56IsOQ0rHnH"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 100\n",
        "BATCH_SIZE = 1024\n",
        "#BATCH_SIZE = 64 #ECU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpkax2oeoa2D"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def make_tf_dataset(dataframe, negative_ratio=None, batch_size=None):\n",
        "\n",
        "    dataset = dataframe.drop(['Unnamed: 0'], axis=1)#, 'SrcGap', 'DstGap', 'DIntPktAct','sMinPktSz', 'Trans'\n",
        "    # Class balancing\n",
        "    pos_df = dataset[dataset['Label'] == 1]\n",
        "    neg_df = dataset[dataset['Label'] == 0]\n",
        "    if negative_ratio:\n",
        "        neg_df = neg_df.iloc[random.sample(range(0, len(neg_df)), len(pos_df)*negative_ratio), :]\n",
        "    balanced_df = pd.concat([pos_df, neg_df], ignore_index=True, sort=False)\n",
        "\n",
        "    y = balanced_df.pop('Label')\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((balanced_df.values, y.to_frame().values))\n",
        "    dataset = dataset.shuffle(4048, seed=SEED) #2048\n",
        "    if batch_size:\n",
        "        dataset = dataset.batch(batch_size)\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXhGlLNzuedf"
      },
      "outputs": [],
      "source": [
        "SEED= 1337"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-gXWNqB8BOL"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "train_data, val_data = [], []\n",
        "\n",
        "for client_data in [client1_shuffled, client2_shuffled, client3_shuffled, client4_shuffled, client5_shuffled,\n",
        "                    client6_shuffled, client7_shuffled, client8_shuffled, client9_shuffled, client10_shuffled]:\n",
        "\n",
        "\n",
        "    train_df, val_df = train_test_split(client_data, test_size=0.1, random_state=1337)\n",
        "\n",
        "    # Scaling (Standardization actually hurts performance)\n",
        "    encoder = LabelEncoder()\n",
        "    scaler = StandardScaler()\n",
        "    train_features = scaler.fit_transform(train_df.drop(['Label'], axis=1))\n",
        "    val_features = scaler.transform(val_df.drop(['Label'], axis=1))\n",
        "\n",
        "    train_df[train_df.columns.difference(['Label'])] = train_features\n",
        "    val_df[val_df.columns.difference(['Label'])] = val_features\n",
        "\n",
        "    # TF Datasets\n",
        "    train_data.append(make_tf_dataset(train_df,negative_ratio=2, batch_size=BATCH_SIZE)) #negative_ratio=2\n",
        "    val_data.append(make_tf_dataset(val_df, batch_size=16))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9svKypU8yYq"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.metrics import CategoricalAccuracy, Precision, Recall, BinaryAccuracy\n",
        "def input_spec():\n",
        "    return (\n",
        "        tf.TensorSpec([None, 35], tf.float64),\n",
        "        tf.TensorSpec([None, 1], tf.int64)\n",
        "    )\n",
        "\n",
        "def model_fn():\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.InputLayer(input_shape=(35,)),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dense(32, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid'),\n",
        "    ])\n",
        "\n",
        "    return tff.learning.from_keras_model(\n",
        "        model,\n",
        "        input_spec=input_spec(),\n",
        "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "        metrics=[tf.keras.metrics.BinaryAccuracy(), Precision(), Recall()]\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-FMfmf69eL0"
      },
      "outputs": [],
      "source": [
        "trainer = tff.learning.build_federated_averaging_process(\n",
        "    model_fn,\n",
        "    client_optimizer_fn=lambda: tf.keras.optimizers.Adam(learning_rate=0.01),\n",
        "    server_optimizer_fn=lambda: tf.keras.optimizers.Adam(learning_rate=0.05)#learning_rate=0.01\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GPjkNjR-OYl"
      },
      "outputs": [],
      "source": [
        "str(trainer.initialize.type_signature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9FcQwx2-zv9"
      },
      "outputs": [],
      "source": [
        "state = trainer.initialize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AgrcITc787Er"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "start = time.time()\n",
        "start_time = time.time()\n",
        "end = time.time()\n",
        "diff=end-start\n",
        "starttest = time.time()\n",
        "endtest =time.time()\n",
        "difftest = endtest-starttest\n",
        "#state = trainer.initialize()\n",
        "train_hist = []\n",
        "for i in range(EPOCHS):\n",
        "    state, metrics = trainer.next(state, train_data)\n",
        "    train_hist.append(metrics)\n",
        "\n",
        "    print(f\"\\rRun {i+1}/{EPOCHS}\", end=\"\")\n",
        "endtest =time.time()\n",
        "#difftest = endtest-starttest\n",
        "print(\"\\nTraining time: \" + str(diff))\n",
        "print(\"\\nTest time: \" + str(difftest))\n",
        "time_required = time.time() - start_time\n",
        "print('\\nTIME: {} seconds'.format(time_required))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pqcwQdUwM8N"
      },
      "outputs": [],
      "source": [
        "evaluator = tff.learning.build_federated_evaluation(model_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgM8AcDiyQrt"
      },
      "outputs": [],
      "source": [
        "#100 epoch\n",
        "federated_metrics = evaluator(state.model, val_data)\n",
        "federated_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVpeR5lu_WDn"
      },
      "outputs": [],
      "source": [
        "print(metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-i0muQ-_ZYV"
      },
      "outputs": [],
      "source": [
        "train_hist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZ1pwE47f2hy"
      },
      "source": [
        "# ***Five Clients***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGqh_fB0sfxq"
      },
      "outputs": [],
      "source": [
        "client1 = pd.read_csv('/content/drive/MyDrive/Five_clients/wus1.csv')\n",
        "client1_shuffled = client1.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "client2 = pd.read_csv('/content/drive/MyDrive/Five_clients/wus2.csv')\n",
        "client2_shuffled = client2.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "client3 = pd.read_csv('/content/drive/MyDrive/Five_clients/wus3.csv')\n",
        "client3_shuffled = client3.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "client4 = pd.read_csv('/content/drive/MyDrive/Five_clients/wus4.csv')\n",
        "client4_shuffled = client4.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "client5 = pd.read_csv('/content/drive/MyDrive/Five_clients/wus5.csv')\n",
        "client5_shuffled = client5.sample(frac=1, random_state = 13).reset_index(drop = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IsgNRtWJsoTW"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 100\n",
        "BATCH_SIZE = 1024\n",
        "#BATCH_SIZE = 64 #ECU\n",
        "SEED=1337"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rg8G9h7Mr8lc"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def make_tf_dataset(dataframe, negative_ratio=None, batch_size=None):\n",
        "\n",
        "    dataset = dataframe.drop(['Unnamed: 0'], axis=1)#, 'SrcGap', 'DstGap', 'DIntPktAct','sMinPktSz', 'Trans'\n",
        "    # Class balancing\n",
        "    pos_df = dataset[dataset['Label'] == 1]\n",
        "    neg_df = dataset[dataset['Label'] == 0]\n",
        "    if negative_ratio:\n",
        "        neg_df = neg_df.iloc[random.sample(range(0, len(neg_df)), len(pos_df)*negative_ratio), :]\n",
        "    balanced_df = pd.concat([pos_df, neg_df], ignore_index=True, sort=False)\n",
        "\n",
        "    y = balanced_df.pop('Label')\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((balanced_df.values, y.to_frame().values))\n",
        "    dataset = dataset.shuffle(4048, seed=SEED) #2048\n",
        "    if batch_size:\n",
        "        dataset = dataset.batch(batch_size)\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBGUN8COg9a9"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "train_data, val_data = [], []\n",
        "\n",
        "for client_data in [client1_shuffled, client2_shuffled, client3_shuffled, client4_shuffled, client5_shuffled]:\n",
        "\n",
        "\n",
        "    train_df, val_df = train_test_split(client_data, test_size=0.1, random_state=1337)\n",
        "\n",
        "    # Scaling (Standardization actually hurts performance)\n",
        "    encoder = LabelEncoder()\n",
        "    scaler = StandardScaler()\n",
        "    train_features = scaler.fit_transform(train_df.drop(['Label'], axis=1))\n",
        "    val_features = scaler.transform(val_df.drop(['Label'], axis=1))\n",
        "\n",
        "    train_df[train_df.columns.difference(['Label'])] = train_features\n",
        "    val_df[val_df.columns.difference(['Label'])] = val_features\n",
        "\n",
        "    # TF Datasets\n",
        "    train_data.append(make_tf_dataset(train_df,negative_ratio=2, batch_size=BATCH_SIZE)) #negative_ratio=2\n",
        "    val_data.append(make_tf_dataset(val_df, batch_size=16))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5p1uvjnhPrA"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.metrics import CategoricalAccuracy, Precision, Recall, BinaryAccuracy\n",
        "def input_spec():\n",
        "    return (\n",
        "        tf.TensorSpec([None, 35], tf.float64),\n",
        "        tf.TensorSpec([None, 1], tf.int64)\n",
        "    )\n",
        "\n",
        "def model_fn():\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.InputLayer(input_shape=(35,)),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dense(32, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid'),\n",
        "    ])\n",
        "\n",
        "    return tff.learning.from_keras_model(\n",
        "        model,\n",
        "        input_spec=input_spec(),\n",
        "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "        metrics=[tf.keras.metrics.BinaryAccuracy(), Precision(), Recall()]\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHhATwYHhVJd"
      },
      "outputs": [],
      "source": [
        "trainer = tff.learning.build_federated_averaging_process(\n",
        "    model_fn,\n",
        "    client_optimizer_fn=lambda: tf.keras.optimizers.Adam(learning_rate=0.01),\n",
        "    server_optimizer_fn=lambda: tf.keras.optimizers.Adam(learning_rate=0.05)#learning_rate=0.01\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5lI0fNurzS6"
      },
      "outputs": [],
      "source": [
        "state = trainer.initialize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1HXU85OsBha"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "start = time.time()\n",
        "start_time = time.time()\n",
        "end = time.time()\n",
        "diff=end-start\n",
        "starttest = time.time()\n",
        "endtest =time.time()\n",
        "difftest = endtest-starttest\n",
        "#state = trainer.initialize()\n",
        "train_hist = []\n",
        "for i in range(EPOCHS):\n",
        "    state, metrics = trainer.next(state, train_data)\n",
        "    train_hist.append(metrics)\n",
        "\n",
        "    print(f\"\\rRun {i+1}/{EPOCHS}\", end=\"\")\n",
        "endtest =time.time()\n",
        "#difftest = endtest-starttest\n",
        "print(\"\\nTraining time: \" + str(diff))\n",
        "print(\"Test time: \" + str(difftest))\n",
        "time_required = time.time() - start_time\n",
        "print('\\nTIME: {}seconds'.format(time_required))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IiKXq69Ji5Jb"
      },
      "outputs": [],
      "source": [
        "evaluator = tff.learning.build_federated_evaluation(model_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snI6n488jAlJ"
      },
      "outputs": [],
      "source": [
        "#100 epoch\n",
        "federated_metrics = evaluator(state.model, val_data)\n",
        "federated_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uST1dXu-PmMm"
      },
      "outputs": [],
      "source": [
        "print(metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EF141fZ_PpT2"
      },
      "outputs": [],
      "source": [
        "train_hist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nU9JNAqJvXW2"
      },
      "source": [
        "# ***ECU-IoHT-Dataset***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "kwNKaKcBvytE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "df_ecu = pd.read_csv('/content/drive/MyDrive/ECU_IoHT.csv')\n",
        "df_old = df_ecu.copy()\n",
        "le = LabelEncoder()\n",
        "df_old['Source'] = le.fit_transform(df_old['Source'])\n",
        "df_old['Destination'] = le.fit_transform(df_old['Destination'])\n",
        "df_old['Protocol'] = le.fit_transform(df_old['Protocol'])\n",
        "df_old['Type'] = le.fit_transform(df_old['Type'])\n",
        "df_old['Type of attack'] = le.fit_transform(df_old['Type of attack'])\n",
        "\n",
        "df_ecu = df_old[['No.', 'Time', 'Source', 'Destination', 'Protocol', 'Length', 'Type', 'Type of attack']].copy()\n",
        "\n",
        "df_ecu.columns = ['Unnamed: 0', 'Time', 'Source', 'Destination', 'Protocol', 'Length', 'Type', 'Type of attack']\n",
        "\n",
        "df_ecu['Time'] = (df_ecu['Time'] * 10).apply(lambda x: round(x))\n",
        "\n",
        "\n",
        "for col in df_ecu.columns:\n",
        "    if col not in ['Type', 'Unnamed: 0']:\n",
        "        df_ecu[col] = df_ecu[col].astype(float).round(1)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "fiew6uOdv6A8",
        "outputId": "c947da6f-54fa-42ce-ac5d-53275421ae74"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_ecu"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-df2cb638-2bd6-4aee-b821-b82d0ea7e03b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Time</th>\n",
              "      <th>Source</th>\n",
              "      <th>Destination</th>\n",
              "      <th>Protocol</th>\n",
              "      <th>Length</th>\n",
              "      <th>Type</th>\n",
              "      <th>Type of attack</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>67.0</td>\n",
              "      <td>69.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>64.0</td>\n",
              "      <td>68.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>2.0</td>\n",
              "      <td>67.0</td>\n",
              "      <td>69.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>2.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>85.0</td>\n",
              "      <td>1</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>4.0</td>\n",
              "      <td>64.0</td>\n",
              "      <td>68.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-df2cb638-2bd6-4aee-b821-b82d0ea7e03b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-df2cb638-2bd6-4aee-b821-b82d0ea7e03b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-df2cb638-2bd6-4aee-b821-b82d0ea7e03b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-2fcdd9be-a9cc-46f9-ae05-fdb0b305d4b2\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2fcdd9be-a9cc-46f9-ae05-fdb0b305d4b2')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-2fcdd9be-a9cc-46f9-ae05-fdb0b305d4b2 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "   Unnamed: 0  Time  Source  Destination  Protocol  Length  Type  \\\n",
              "0           1   0.0    67.0         69.0       0.0    42.0     0   \n",
              "1           2   0.0    64.0         68.0       0.0    42.0     0   \n",
              "2           3   2.0    67.0         69.0       0.0    42.0     0   \n",
              "3           4   2.0    45.0         44.0       2.0    85.0     1   \n",
              "4           5   4.0    64.0         68.0       0.0    42.0     0   \n",
              "\n",
              "   Type of attack  \n",
              "0             0.0  \n",
              "1             0.0  \n",
              "2             0.0  \n",
              "3             3.0  \n",
              "4             0.0  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_ecu.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "jVHog77LwB1t",
        "outputId": "912b6752-0415-4657-903e-c94846c79072"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Type</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>87754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>23453</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ],
            "text/plain": [
              "Type\n",
              "0    87754\n",
              "1    23453\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_ecu.Type.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98wS9KSNwUQs",
        "outputId": "3f13adc4-6525-4684-8799-13d69f406467"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
            "  return bound(*args, **kwds)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "for i, df in enumerate(np.array_split(df_ecu, 5)):\n",
        "    df.to_csv('/content/drive/MyDrive/'+f\"ecu{i+1}.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYwhaDYRzbAW"
      },
      "source": [
        "# ***Fifteen Clients***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEgbNhOZzanq"
      },
      "outputs": [],
      "source": [
        "client1_ECU = pd.read_csv('/content/drive/MyDrive/15-ecu-clients/ecu1.csv')\n",
        "client1_ECU_shuffled = client1_ECU.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "client2_ECU = pd.read_csv('/content/drive/MyDrive/15-ecu-clients/ecu2.csv')\n",
        "client2_ECU_shuffled = client2_ECU.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "client3_ECU = pd.read_csv('/content/drive/MyDrive/15-ecu-clients/ecu3.csv')\n",
        "client3_ECU_shuffled = client3_ECU.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "client4_ECU = pd.read_csv('/content/drive/MyDrive/15-ecu-clients/ecu4.csv')\n",
        "client4_ECU_shuffled = client4_ECU.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "client5_ECU = pd.read_csv('/content/drive/MyDrive/15-ecu-clients/ecu5.csv')\n",
        "client5_ECU_shuffled = client5_ECU.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "client6_ECU = pd.read_csv('/content/drive/MyDrive/15-ecu-clients/ecu6.csv')\n",
        "client6_ECU_shuffled = client6_ECU.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "client7_ECU = pd.read_csv('/content/drive/MyDrive/15-ecu-clients/ecu7.csv')\n",
        "client7_ECU_shuffled = client7_ECU.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "client8_ECU = pd.read_csv('/content/drive/MyDrive/15-ecu-clients/ecu8.csv')\n",
        "client8_ECU_shuffled = client8_ECU.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "client9_ECU = pd.read_csv('/content/drive/MyDrive/15-ecu-clients/ecu9.csv')\n",
        "client9_ECU_shuffled = client9_ECU.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "client10_ECU = pd.read_csv('/content/drive/MyDrive/15-ecu-clients/ecu10.csv')\n",
        "client10_ECU_shuffled = client10_ECU.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "client11_ECU = pd.read_csv('/content/drive/MyDrive/15-ecu-clients/ecu11.csv')\n",
        "client11_ECU_shuffled = client11_ECU.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "client12_ECU = pd.read_csv('/content/drive/MyDrive/15-ecu-clients/ecu12.csv')\n",
        "client12_ECU_shuffled = client12_ECU.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "client13_ECU = pd.read_csv('/content/drive/MyDrive/15-ecu-clients/ecu13.csv')\n",
        "client13_ECU_shuffled = client13_ECU.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "client14_ECU = pd.read_csv('/content/drive/MyDrive/15-ecu-clients/ecu14.csv')\n",
        "client14_ECU_shuffled = client14_ECU.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "client15_ECU = pd.read_csv('/content/drive/MyDrive/15-ecu-clients/ecu15.csv')\n",
        "client15_ECU_shuffled = client15_ECU.sample(frac=1, random_state = 13).reset_index(drop = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfyoGJM65530"
      },
      "outputs": [],
      "source": [
        "client13_ECU_shuffled.Type.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JkuDtgZv3SlZ"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 100\n",
        "#BATCH_SIZE = 512\n",
        "BATCH_SIZE = 1024 #ECU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJM9B3cl3YkM"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def make_tf_dataset(dataframe, negative_ratio=None, batch_size=None):\n",
        "\n",
        "    dataset = dataframe.drop(['Unnamed: 0'], axis=1)#, 'SrcGap', 'DstGap', 'DIntPktAct','sMinPktSz', 'Trans'\n",
        "    # Class balancing\n",
        "    pos_df = dataset[dataset['Type'] == 1]\n",
        "    neg_df = dataset[dataset['Type'] == 0]\n",
        "    if negative_ratio:\n",
        "        neg_df = neg_df.iloc[random.sample(range(0, len(neg_df)), len(pos_df)*negative_ratio), :]\n",
        "    balanced_df = pd.concat([pos_df, neg_df], ignore_index=True, sort=False)\n",
        "\n",
        "    y = balanced_df.pop('Type')\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((balanced_df.values, y.to_frame().values))\n",
        "    dataset = dataset.shuffle(4048, seed=SEED) #2048\n",
        "    if batch_size:\n",
        "        dataset = dataset.batch(batch_size)\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mRSrNyEz3dgi"
      },
      "outputs": [],
      "source": [
        "#########################################ECU#######################################################\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "train_data, val_data = [], []\n",
        "\n",
        "for client_data in [client1_ECU_shuffled, client2_ECU_shuffled, client3_ECU_shuffled, client4_ECU_shuffled, client5_ECU_shuffled,\n",
        "                    client6_ECU_shuffled, client7_ECU_shuffled, client8_ECU_shuffled, client9_ECU_shuffled, client10_ECU_shuffled,\n",
        "                    client11_ECU_shuffled, client12_ECU_shuffled, client13_ECU_shuffled, client14_ECU_shuffled, client15_ECU_shuffled]:\n",
        "\n",
        "\n",
        "    train_df, val_df = train_test_split(client_data, test_size=0.2, random_state=1337)\n",
        "\n",
        "    # Scaling (Standardization actually hurts performance)\n",
        "    #encoder = LabelEncoder()\n",
        "    scaler = StandardScaler()\n",
        "    train_features = scaler.fit_transform(train_df.drop(['Type'], axis=1))\n",
        "    val_features = scaler.transform(val_df.drop(['Type'], axis=1))\n",
        "\n",
        "    #encoder = LabelEncoder()\n",
        "    #y1 = encoder.fit_transform(y)\n",
        "    #Y= pd.get_dummies(y1).values\n",
        "    train_df[train_df.columns.difference(['Type'])] = train_features\n",
        "    val_df[val_df.columns.difference(['Type'])] = val_features\n",
        "\n",
        "    # TF Datasets\n",
        "    train_data.append(make_tf_dataset(train_df, batch_size=BATCH_SIZE)) #negative_ratio=2\n",
        "    val_data.append(make_tf_dataset(val_df, batch_size=16))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YO8fasDu3rvl"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.metrics import CategoricalAccuracy, Precision, Recall, BinaryAccuracy\n",
        "def input_spec():\n",
        "    return (\n",
        "        tf.TensorSpec([None, 6], tf.float64),\n",
        "        tf.TensorSpec([None, 1], tf.int64)\n",
        "    )\n",
        "\n",
        "def model_fn():\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.InputLayer(input_shape=(6,)),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dense(32, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid'),\n",
        "    ])\n",
        "\n",
        "    return tff.learning.from_keras_model(\n",
        "        model,\n",
        "        input_spec=input_spec(),\n",
        "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "        metrics=[tf.keras.metrics.BinaryAccuracy(), Precision(), Recall()]\n",
        "        )\n",
        "trainer = tff.learning.build_federated_averaging_process(\n",
        "    model_fn,\n",
        "    client_optimizer_fn=lambda: tf.keras.optimizers.Adam(learning_rate=0.01),\n",
        "    server_optimizer_fn=lambda: tf.keras.optimizers.Adam(learning_rate=0.05)#learning_rate=0.01\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEoA3YeQ6u8r"
      },
      "outputs": [],
      "source": [
        "trainer = tff.learning.build_federated_averaging_process(\n",
        "    model_fn,\n",
        "    client_optimizer_fn=lambda: tf.keras.optimizers.Adam(learning_rate=0.01),\n",
        "    server_optimizer_fn=lambda: tf.keras.optimizers.Adam(learning_rate=0.05)#learning_rate=0.01\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l12z8-Tn6x7R"
      },
      "outputs": [],
      "source": [
        "str(trainer.initialize.type_signature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3m-MUfy60lX"
      },
      "outputs": [],
      "source": [
        "state = trainer.initialize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6rT8qQf63wF"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "start = time.time()\n",
        "start_time = time.time()\n",
        "end = time.time()\n",
        "diff=end-start\n",
        "starttest = time.time()\n",
        "endtest =time.time()\n",
        "difftest = endtest-starttest\n",
        "#state = trainer.initialize()\n",
        "train_hist = []\n",
        "for i in range(EPOCHS):\n",
        "    state, metrics = trainer.next(state, train_data)\n",
        "    train_hist.append(metrics)\n",
        "\n",
        "    print(f\"\\rRun {i+1}/{EPOCHS}\", end=\"\")\n",
        "endtest =time.time()\n",
        "#difftest = endtest-starttest\n",
        "print(\"\\nTraining time: \" + str(diff))\n",
        "print(\"Test time: \" + str(difftest))\n",
        "time_required = time.time() - start_time\n",
        "print('\\nTIME: {}seconds'.format(time_required))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AX88TDUa7kFG"
      },
      "outputs": [],
      "source": [
        "evaluator = tff.learning.build_federated_evaluation(model_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vd6T4Hw7mXs"
      },
      "outputs": [],
      "source": [
        "federated_metrics = evaluator(state.model, val_data)\n",
        "federated_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWHrc3cX7pK2"
      },
      "outputs": [],
      "source": [
        "print(metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAbjlVdI7q-S"
      },
      "outputs": [],
      "source": [
        "train_hist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMdSjhqU2i1o"
      },
      "source": [
        "# ***Ten clients***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpjDulpG20Pa"
      },
      "outputs": [],
      "source": [
        "client1_ECU = pd.read_csv('/content/drive/MyDrive/10-ecu-clients/ecu1.csv')\n",
        "client1_ECU_shuffled = client1_ECU.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "client2_ECU = pd.read_csv('/content/drive/MyDrive/10-ecu-clients/ecu2.csv')\n",
        "client2_ECU_shuffled = client2_ECU.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "client3_ECU = pd.read_csv('/content/drive/MyDrive/10-ecu-clients/ecu3.csv')\n",
        "client3_ECU_shuffled = client3_ECU.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "client4_ECU = pd.read_csv('/content/drive/MyDrive/10-ecu-clients/ecu4.csv')\n",
        "client4_ECU_shuffled = client4_ECU.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "client5_ECU = pd.read_csv('/content/drive/MyDrive/10-ecu-clients/ecu5.csv')\n",
        "client5_ECU_shuffled = client5_ECU.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "client6_ECU = pd.read_csv('/content/drive/MyDrive/10-ecu-clients/ecu6.csv')\n",
        "client6_ECU_shuffled = client6_ECU.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "client7_ECU = pd.read_csv('/content/drive/MyDrive/10-ecu-clients/ecu7.csv')\n",
        "client7_ECU_shuffled = client7_ECU.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "client8_ECU = pd.read_csv('/content/drive/MyDrive/10-ecu-clients/ecu8.csv')\n",
        "client8_ECU_shuffled = client8_ECU.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "client9_ECU = pd.read_csv('/content/drive/MyDrive/10-ecu-clients/ecu9.csv')\n",
        "client9_ECU_shuffled = client9_ECU.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "client10_ECU = pd.read_csv('/content/drive/MyDrive/10-ecu-clients/ecu10.csv')\n",
        "client10_ECU_shuffled = client10_ECU.sample(frac=1, random_state = 13).reset_index(drop = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlDLT7Vz-t99"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 100\n",
        "BATCH_SIZE = 1024\n",
        "#BATCH_SIZE = 64 #ECU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5nud_Z1-1Bd"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def make_tf_dataset(dataframe, negative_ratio=None, batch_size=None):\n",
        "\n",
        "    dataset = dataframe.drop(['Unnamed: 0'], axis=1)#, 'SrcGap', 'DstGap', 'DIntPktAct','sMinPktSz', 'Trans'\n",
        "    # Class balancing\n",
        "    pos_df = dataset[dataset['Type'] == 1]\n",
        "    neg_df = dataset[dataset['Type'] == 0]\n",
        "    if negative_ratio:\n",
        "        neg_df = neg_df.iloc[random.sample(range(0, len(neg_df)), len(pos_df)*negative_ratio), :]\n",
        "    balanced_df = pd.concat([pos_df, neg_df], ignore_index=True, sort=False)\n",
        "\n",
        "    y = balanced_df.pop('Type')\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((balanced_df.values, y.to_frame().values))\n",
        "    dataset = dataset.shuffle(4048, seed=SEED) #2048\n",
        "    if batch_size:\n",
        "        dataset = dataset.batch(batch_size)\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Gr2YxUdAOXw"
      },
      "outputs": [],
      "source": [
        "SEED= 1337"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7a96N4Tz-4nT"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "train_data, val_data = [], []\n",
        "\n",
        "for client_data in [client1_ECU_shuffled, client2_ECU_shuffled, client3_ECU_shuffled, client4_ECU_shuffled, client5_ECU_shuffled,\n",
        "                    client6_ECU_shuffled, client7_ECU_shuffled, client8_ECU_shuffled, client9_ECU_shuffled, client10_ECU_shuffled]:\n",
        "\n",
        "\n",
        "    train_df, val_df = train_test_split(client_data, test_size=0.1, random_state=1337)\n",
        "\n",
        "    # Scaling (Standardization actually hurts performance)\n",
        "    encoder = LabelEncoder()\n",
        "    scaler = StandardScaler()\n",
        "    train_features = scaler.fit_transform(train_df.drop(['Type'], axis=1))\n",
        "    val_features = scaler.transform(val_df.drop(['Type'], axis=1))\n",
        "\n",
        "    train_df[train_df.columns.difference(['Type'])] = train_features\n",
        "    val_df[val_df.columns.difference(['Type'])] = val_features\n",
        "\n",
        "    # TF Datasets\n",
        "    train_data.append(make_tf_dataset(train_df, negative_ratio=0, batch_size=BATCH_SIZE)) #negative_ratio=2\n",
        "    val_data.append(make_tf_dataset(val_df, batch_size=16))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-taWsgIAVbC"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.metrics import CategoricalAccuracy, Precision, Recall, BinaryAccuracy\n",
        "def input_spec():\n",
        "    return (\n",
        "        tf.TensorSpec([None, 6], tf.float64),\n",
        "        tf.TensorSpec([None, 1], tf.int64)\n",
        "    )\n",
        "\n",
        "def model_fn():\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.InputLayer(input_shape=(6,)),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dense(32, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid'),\n",
        "    ])\n",
        "\n",
        "    return tff.learning.from_keras_model(\n",
        "        model,\n",
        "        input_spec=input_spec(),\n",
        "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "        metrics=[tf.keras.metrics.BinaryAccuracy(), Precision(), Recall()]\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "No5z1IH4Aqwk"
      },
      "outputs": [],
      "source": [
        "trainer = tff.learning.build_federated_averaging_process(\n",
        "    model_fn,\n",
        "    client_optimizer_fn=lambda: tf.keras.optimizers.Adam(learning_rate=0.01),\n",
        "    server_optimizer_fn=lambda: tf.keras.optimizers.Adam(learning_rate=0.05)#learning_rate=0.01\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OEB92l8EAs9B"
      },
      "outputs": [],
      "source": [
        "str(trainer.initialize.type_signature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTA0Rq9XAvwI"
      },
      "outputs": [],
      "source": [
        "state = trainer.initialize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4oCdH16Ay3e"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "start = time.time()\n",
        "start_time = time.time()\n",
        "end = time.time()\n",
        "diff=end-start\n",
        "starttest = time.time()\n",
        "endtest =time.time()\n",
        "difftest = endtest-starttest\n",
        "#state = trainer.initialize()\n",
        "train_hist = []\n",
        "for i in range(EPOCHS):\n",
        "    state, metrics = trainer.next(state, train_data)\n",
        "    train_hist.append(metrics)\n",
        "\n",
        "    print(f\"\\rRun {i+1}/{EPOCHS}\", end=\"\")\n",
        "endtest =time.time()\n",
        "#difftest = endtest-starttest\n",
        "print(\"\\nTraining time: \" + str(diff))\n",
        "print(\"Test time: \" + str(difftest))\n",
        "time_required = time.time() - start_time\n",
        "print('\\nTIME: {}seconds'.format(time_required))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yCv4yxRA-Op"
      },
      "outputs": [],
      "source": [
        "evaluator = tff.learning.build_federated_evaluation(model_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_W4Sj3rfA4rn"
      },
      "outputs": [],
      "source": [
        "evaluator = tff.learning.build_federated_evaluation(model_fn)\n",
        "federated_metrics = evaluator(state.model, val_data)\n",
        "federated_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMmlgHp0BAwB"
      },
      "outputs": [],
      "source": [
        "print(metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_27P97lFBCiI"
      },
      "outputs": [],
      "source": [
        "train_hist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0Dp5MLX22xs"
      },
      "source": [
        "# ***Five clients***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "KffI4zZ827YF"
      },
      "outputs": [],
      "source": [
        "client1_ECU = pd.read_csv('/content/drive/MyDrive/ecu1.csv')\n",
        "client1_ECU_shuffled = client1_ECU.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "client2_ECU = pd.read_csv('/content/drive/MyDrive/ecu2.csv')\n",
        "client2_ECU_shuffled = client2_ECU.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "client3_ECU = pd.read_csv('/content/drive/MyDrive/ecu3.csv')\n",
        "client3_ECU_shuffled = client3_ECU.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "client4_ECU = pd.read_csv('/content/drive/MyDrive/ecu4.csv')\n",
        "client4_ECU_shuffled = client4_ECU.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "client5_ECU = pd.read_csv('/content/drive/MyDrive/ecu5.csv')\n",
        "client5_ECU_shuffled = client5_ECU.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_Y7-Uglr-vMR"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 100\n",
        "#BATCH_SIZE = 1024\n",
        "BATCH_SIZE = 64 #ECU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "cilUg5EP-xmc"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def make_tf_dataset(dataframe, negative_ratio=None, batch_size=None):\n",
        "\n",
        "    dataset = dataframe.drop(['Unnamed: 0'], axis=1)#, 'SrcGap', 'DstGap', 'DIntPktAct','sMinPktSz', 'Trans'\n",
        "    # Class balancing\n",
        "    pos_df = dataset[dataset['Type'] == 1]\n",
        "    neg_df = dataset[dataset['Type'] == 0]\n",
        "    if negative_ratio:\n",
        "        neg_df = neg_df.iloc[random.sample(range(0, len(neg_df)), len(pos_df)*negative_ratio), :]\n",
        "    balanced_df = pd.concat([pos_df, neg_df], ignore_index=True, sort=False)\n",
        "\n",
        "    y = balanced_df.pop('Type')\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((balanced_df.values, y.to_frame().values))\n",
        "    dataset = dataset.shuffle(4048, seed=SEED) #2048\n",
        "    if batch_size:\n",
        "        dataset = dataset.batch(batch_size)\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "hGGpzdWdcK9v"
      },
      "outputs": [],
      "source": [
        "SEED=1337"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "N2xjyYLt-6IJ"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "train_data, val_data = [], []\n",
        "\n",
        "for client_data in [client1_ECU_shuffled, client2_ECU_shuffled, client3_ECU_shuffled, client4_ECU_shuffled, client5_ECU_shuffled]:\n",
        "\n",
        "\n",
        "    train_df, val_df = train_test_split(client_data, test_size=0.1, random_state=1337)\n",
        "\n",
        "    # Scaling (Standardization actually hurts performance)\n",
        "    encoder = LabelEncoder()\n",
        "    scaler = StandardScaler()\n",
        "    train_features = scaler.fit_transform(train_df.drop(['Type'], axis=1))\n",
        "    val_features = scaler.transform(val_df.drop(['Type'], axis=1))\n",
        "\n",
        "    train_df[train_df.columns.difference(['Type'])] = train_features\n",
        "    val_df[val_df.columns.difference(['Type'])] = val_features\n",
        "\n",
        "    # TF Datasets\n",
        "    train_data.append(make_tf_dataset(train_df,negative_ratio=0, batch_size=BATCH_SIZE)) #negative_ratio=2\n",
        "    val_data.append(make_tf_dataset(val_df, batch_size=16))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "B7jCNzWHcUoj"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.metrics import CategoricalAccuracy, Precision, Recall, BinaryAccuracy\n",
        "def input_spec():\n",
        "    return (\n",
        "        tf.TensorSpec([None, 6], tf.float64),\n",
        "        tf.TensorSpec([None, 1], tf.int64)\n",
        "    )\n",
        "\n",
        "def model_fn():\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.InputLayer(input_shape=(6,)),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dense(32, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid'),\n",
        "    ])\n",
        "\n",
        "    return tff.learning.from_keras_model(\n",
        "        model,\n",
        "        input_spec=input_spec(),\n",
        "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "        metrics=[tf.keras.metrics.BinaryAccuracy(), Precision(), Recall()]\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "As6Bhrqwpc-d",
        "outputId": "a7b7ae7d-ed8a-4edb-f885-d532a6b379af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-federated in /usr/local/lib/python3.10/dist-packages (0.86.0)\n",
            "Requirement already satisfied: absl-py==1.*,>=1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-federated) (1.4.0)\n",
            "Requirement already satisfied: attrs~=23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-federated) (23.1.0)\n",
            "Requirement already satisfied: cachetools~=5.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-federated) (5.5.0)\n",
            "Requirement already satisfied: dm-tree==0.1.8 in /usr/local/lib/python3.10/dist-packages (from tensorflow-federated) (0.1.8)\n",
            "Requirement already satisfied: dp-accounting==0.4.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-federated) (0.4.3)\n",
            "Requirement already satisfied: google-vizier==0.1.11 in /usr/local/lib/python3.10/dist-packages (from tensorflow-federated) (0.1.11)\n",
            "Requirement already satisfied: grpcio~=1.46 in /usr/local/lib/python3.10/dist-packages (from tensorflow-federated) (1.64.1)\n",
            "Requirement already satisfied: jaxlib==0.4.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow-federated) (0.4.14)\n",
            "Requirement already satisfied: jax==0.4.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow-federated) (0.4.14)\n",
            "Requirement already satisfied: ml-dtypes==0.2.*,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-federated) (0.2.0)\n",
            "Requirement already satisfied: numpy~=1.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow-federated) (1.25.2)\n",
            "Requirement already satisfied: portpicker~=1.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow-federated) (1.6.0)\n",
            "Requirement already satisfied: scipy~=1.9.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-federated) (1.9.3)\n",
            "Requirement already satisfied: tensorflow-model-optimization==0.7.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow-federated) (0.7.5)\n",
            "Requirement already satisfied: tensorflow-privacy==0.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-federated) (0.9.0)\n",
            "Requirement already satisfied: tensorflow==2.14.*,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-federated) (2.14.1)\n",
            "Requirement already satisfied: tqdm~=4.64 in /usr/local/lib/python3.10/dist-packages (from tensorflow-federated) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions==4.5.*,>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-federated) (4.5.0)\n",
            "Requirement already satisfied: googleapis-common-protos==1.61.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-federated) (1.61.0)\n",
            "Requirement already satisfied: mpmath~=1.2 in /usr/local/lib/python3.10/dist-packages (from dp-accounting==0.4.3->tensorflow-federated) (1.3.0)\n",
            "Requirement already satisfied: protobuf>=3.6 in /usr/local/lib/python3.10/dist-packages (from google-vizier==0.1.11->tensorflow-federated) (4.25.4)\n",
            "Requirement already satisfied: grpcio-tools>=1.35.0 in /usr/local/lib/python3.10/dist-packages (from google-vizier==0.1.11->tensorflow-federated) (1.62.3)\n",
            "Requirement already satisfied: sqlalchemy<=1.4.20,>=1.4 in /usr/local/lib/python3.10/dist-packages (from google-vizier==0.1.11->tensorflow-federated) (1.4.20)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax==0.4.14->tensorflow-federated) (3.3.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (18.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (22.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (2.4.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (0.37.1)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (2.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (2.14.0)\n",
            "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (2.14.0)\n",
            "Requirement already satisfied: scikit-learn==1.*,>=1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-privacy==0.9.0->tensorflow-federated) (1.3.2)\n",
            "Requirement already satisfied: tensorflow-probability~=0.22.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-privacy==0.9.0->tensorflow-federated) (0.22.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.*,>=1.0->tensorflow-privacy==0.9.0->tensorflow-federated) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.*,>=1.0->tensorflow-privacy==0.9.0->tensorflow-federated) (3.5.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from portpicker~=1.6->tensorflow-federated) (5.9.5)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (0.44.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<=1.4.20,>=1.4->google-vizier==0.1.11->tensorflow-federated) (3.0.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (3.0.4)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.22.0->tensorflow-privacy==0.9.0->tensorflow-federated) (4.4.2)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.22.0->tensorflow-privacy==0.9.0->tensorflow-federated) (2.2.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade tensorflow-federated\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3weabUNs72D"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kG4j40KocZAh",
        "outputId": "3dd55ab9-c3a6-46db-e9f9-214e112b37c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-federated in /usr/local/lib/python3.10/dist-packages (0.86.0)\n",
            "Requirement already satisfied: absl-py==1.*,>=1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-federated) (1.4.0)\n",
            "Requirement already satisfied: attrs~=23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-federated) (23.1.0)\n",
            "Requirement already satisfied: cachetools~=5.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-federated) (5.5.0)\n",
            "Requirement already satisfied: dm-tree==0.1.8 in /usr/local/lib/python3.10/dist-packages (from tensorflow-federated) (0.1.8)\n",
            "Requirement already satisfied: dp-accounting==0.4.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-federated) (0.4.3)\n",
            "Requirement already satisfied: google-vizier==0.1.11 in /usr/local/lib/python3.10/dist-packages (from tensorflow-federated) (0.1.11)\n",
            "Requirement already satisfied: grpcio~=1.46 in /usr/local/lib/python3.10/dist-packages (from tensorflow-federated) (1.64.1)\n",
            "Requirement already satisfied: jaxlib==0.4.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow-federated) (0.4.14)\n",
            "Requirement already satisfied: jax==0.4.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow-federated) (0.4.14)\n",
            "Requirement already satisfied: ml-dtypes==0.2.*,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-federated) (0.2.0)\n",
            "Requirement already satisfied: numpy~=1.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow-federated) (1.25.2)\n",
            "Requirement already satisfied: portpicker~=1.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow-federated) (1.6.0)\n",
            "Requirement already satisfied: scipy~=1.9.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-federated) (1.9.3)\n",
            "Requirement already satisfied: tensorflow-model-optimization==0.7.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow-federated) (0.7.5)\n",
            "Requirement already satisfied: tensorflow-privacy==0.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-federated) (0.9.0)\n",
            "Requirement already satisfied: tensorflow==2.14.*,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-federated) (2.14.1)\n",
            "Requirement already satisfied: tqdm~=4.64 in /usr/local/lib/python3.10/dist-packages (from tensorflow-federated) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions==4.5.*,>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-federated) (4.5.0)\n",
            "Requirement already satisfied: googleapis-common-protos==1.61.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-federated) (1.61.0)\n",
            "Requirement already satisfied: mpmath~=1.2 in /usr/local/lib/python3.10/dist-packages (from dp-accounting==0.4.3->tensorflow-federated) (1.3.0)\n",
            "Requirement already satisfied: protobuf>=3.6 in /usr/local/lib/python3.10/dist-packages (from google-vizier==0.1.11->tensorflow-federated) (4.25.4)\n",
            "Requirement already satisfied: grpcio-tools>=1.35.0 in /usr/local/lib/python3.10/dist-packages (from google-vizier==0.1.11->tensorflow-federated) (1.62.3)\n",
            "Requirement already satisfied: sqlalchemy<=1.4.20,>=1.4 in /usr/local/lib/python3.10/dist-packages (from google-vizier==0.1.11->tensorflow-federated) (1.4.20)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax==0.4.14->tensorflow-federated) (3.3.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (18.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (22.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (2.4.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (0.37.1)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (2.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (2.14.0)\n",
            "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (2.14.0)\n",
            "Requirement already satisfied: scikit-learn==1.*,>=1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-privacy==0.9.0->tensorflow-federated) (1.3.2)\n",
            "Requirement already satisfied: tensorflow-probability~=0.22.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-privacy==0.9.0->tensorflow-federated) (0.22.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.*,>=1.0->tensorflow-privacy==0.9.0->tensorflow-federated) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.*,>=1.0->tensorflow-privacy==0.9.0->tensorflow-federated) (3.5.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from portpicker~=1.6->tensorflow-federated) (5.9.5)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (0.44.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<=1.4.20,>=1.4->google-vizier==0.1.11->tensorflow-federated) (3.0.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (3.0.4)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.22.0->tensorflow-privacy==0.9.0->tensorflow-federated) (4.4.2)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.22.0->tensorflow-privacy==0.9.0->tensorflow-federated) (2.2.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade tensorflow-federated\n",
        "import tensorflow_federated as tff\n",
        "\n",
        "trainer = tff.learning.algorithms.build_weighted_fed_avg(\n",
        "    model_fn,\n",
        "    client_optimizer_fn=lambda: tf.keras.optimizers.Adam(learning_rate=0.01),\n",
        "    server_optimizer_fn=lambda: tf.keras.optimizers.Adam(learning_rate=0.05)#learning_rate=0.01\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRrV0nmUqdfK"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "st9gmgc_cbV5",
        "outputId": "4264a607-d518-4054-b2a7-2bbfec48c198"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'( -> <global_model_weights=<trainable=<float32[6,64],float32[64],float32[64,64],float32[64],float32[64,32],float32[32],float32[32,1],float32[1]>,non_trainable=<>>,distributor=<>,client_work=<>,aggregator=<value_sum_process=<>,weight_sum_process=<>>,finalizer=<int64,float32[6,64],float32[6,64],float32[64],float32[64],float32[64,64],float32[64,64],float32[64],float32[64],float32[64,32],float32[64,32],float32[32],float32[32],float32[32,1],float32[32,1],float32[1],float32[1]>>@SERVER)'"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "str(trainer.initialize.type_signature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "RATqL4eKcduk"
      },
      "outputs": [],
      "source": [
        "state = trainer.initialize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auW981G7cgxY",
        "outputId": "6ccd01fd-e0cf-4c29-9f13-9aff18bc0e99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Run 100/100\n",
            "Training time: 0.00010752677917480469\n",
            "Test time: 2.4557113647460938e-05\n",
            "\n",
            "TIME: 286.1836585998535seconds\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "start = time.time()\n",
        "start_time = time.time()\n",
        "end = time.time()\n",
        "diff=end-start\n",
        "starttest = time.time()\n",
        "endtest =time.time()\n",
        "difftest = endtest-starttest\n",
        "#state = trainer.initialize()\n",
        "train_hist = []\n",
        "for i in range(EPOCHS):\n",
        "    state, metrics = trainer.next(state, train_data)\n",
        "    train_hist.append(metrics)\n",
        "\n",
        "    print(f\"\\rRun {i+1}/{EPOCHS}\", end=\"\")\n",
        "endtest =time.time()\n",
        "#difftest = endtest-starttest\n",
        "print(\"\\nTraining time: \" + str(diff))\n",
        "print(\"Test time: \" + str(difftest))\n",
        "time_required = time.time() - start_time\n",
        "print('\\nTIME: {}seconds'.format(time_required))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "eOFlZ038cy_5"
      },
      "outputs": [],
      "source": [
        "evaluator = tff.learning.algorithms.build_fed_eval(model_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "4YDDBZ7Mc1GC",
        "outputId": "48ebdc7f-8270-4a85-f89b-c5a56e4ee3b2"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'LearningAlgorithmState' object has no attribute 'model'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-000d77b287e0>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfederated_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfederated_metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'LearningAlgorithmState' object has no attribute 'model'"
          ]
        }
      ],
      "source": [
        "federated_metrics = evaluator(state.model, val_data)\n",
        "federated_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neK5Apg8c4--"
      },
      "outputs": [],
      "source": [
        "print(metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6yYI3UFc68H"
      },
      "outputs": [],
      "source": [
        "train_hist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XPeTFsSA_a_"
      },
      "source": [
        "# ***ICU-Dataset***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsVa0uSVCHbw"
      },
      "outputs": [],
      "source": [
        "df_ICU1 = pd.read_csv('/content/drive/MyDrive/ICU/clean_ICU.csv')\n",
        "df_ICU= df_ICU1.sample(frac=1, random_state = 13).reset_index(drop = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IuJN-WEwDRF_"
      },
      "outputs": [],
      "source": [
        "df_ICU.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPaotfB-vnZj"
      },
      "outputs": [],
      "source": [
        "df_ICU.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Xw4rRuqDe1u"
      },
      "outputs": [],
      "source": [
        "df_ICU.loc[df_ICU['frame.time_delta'] == '0x00000012']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jd7GtfJ7Di8y"
      },
      "outputs": [],
      "source": [
        "col = ['tcp.flags', 'tcp.checksum', 'mqtt.hdrflags', 'tcp.flags.ack', 'tcp.flags.fin']\n",
        "df_ICU= df_ICU.drop(columns = col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QM7URluZDmlw"
      },
      "outputs": [],
      "source": [
        "df_ICU.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbhfoo0gD0XE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "for i, df in enumerate(np.array_split(df_ICU, 5)):\n",
        "    df.to_csv('/content/drive/MyDrive/5-ICU-Env/'+f\"ICU-Env{i+1}.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lx68vSwEqar"
      },
      "source": [
        "# ***Fifteen clients***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUFeKt2rEy7r"
      },
      "outputs": [],
      "source": [
        "ICU1 = pd.read_csv('/content/drive/MyDrive/15-ICU-Env/ICU-Env1.csv')\n",
        "ICU1_shuffled = ICU1.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "ICU2 = pd.read_csv('/content/drive/MyDrive/15-ICU-Env/ICU-Env2.csv')\n",
        "ICU2_shuffled = ICU2.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "ICU3 = pd.read_csv('/content/drive/MyDrive/15-ICU-Env/ICU-Env3.csv')\n",
        "ICU3_shuffled = ICU3.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "ICU4 = pd.read_csv('/content/drive/MyDrive/15-ICU-Env/ICU-Env4.csv')\n",
        "ICU4_shuffled = ICU4.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "ICU5 = pd.read_csv('/content/drive/MyDrive/15-ICU-Env/ICU-Env5.csv')\n",
        "ICU5_shuffled = ICU5.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "ICU6 = pd.read_csv('/content/drive/MyDrive/15-ICU-Env/ICU-Env6.csv')\n",
        "ICU6_shuffled = ICU6.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "ICU7 = pd.read_csv('/content/drive/MyDrive/15-ICU-Env/ICU-Env7.csv')\n",
        "ICU7_shuffled = ICU7.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "ICU8 = pd.read_csv('/content/drive/MyDrive/15-ICU-Env/ICU-Env8.csv')\n",
        "ICU8_shuffled = ICU8.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "ICU9 = pd.read_csv('/content/drive/MyDrive/15-ICU-Env/ICU-Env9.csv')\n",
        "ICU9_shuffled = ICU9.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "ICU10 = pd.read_csv('/content/drive/MyDrive/15-ICU-Env/ICU-Env10.csv')\n",
        "ICU10_shuffled = ICU10.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "ICU11 = pd.read_csv('/content/drive/MyDrive/15-ICU-Env/ICU-Env11.csv')\n",
        "ICU11_shuffled = ICU11.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "ICU12 = pd.read_csv('/content/drive/MyDrive/15-ICU-Env/ICU-Env12.csv')\n",
        "ICU12_shuffled = ICU12.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "ICU13 = pd.read_csv('/content/drive/MyDrive/15-ICU-Env/ICU-Env13.csv')\n",
        "ICU13_shuffled = ICU13.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "ICU14 = pd.read_csv('/content/drive/MyDrive/15-ICU-Env/ICU-Env14.csv')\n",
        "ICU14_shuffled = ICU14.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "ICU15 = pd.read_csv('/content/drive/MyDrive/15-ICU-Env/ICU-Env15.csv')\n",
        "ICU15_shuffled = ICU15.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJJ_eK6bk0ww"
      },
      "outputs": [],
      "source": [
        "ICU15_shuffled.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQANo9SsjyUD"
      },
      "outputs": [],
      "source": [
        "ICU15_shuffled.label.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dz61154MkQnP"
      },
      "outputs": [],
      "source": [
        "ex = pd.read_csv('/content/drive/MyDrive/10-ICU-Env1/ICU-Env1.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAjBIbHbkXuN"
      },
      "outputs": [],
      "source": [
        "ex.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDSmJbhtF7Yh"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 100\n",
        "#BATCH_SIZE = 512\n",
        "BATCH_SIZE = 1024 #ECU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qcIvfhbGDuj"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def make_tf_dataset(dataframe1, batch_size=None):\n",
        "\n",
        "    dataset = dataframe1.drop(['Unnamed: 0', 'frame.time_relative', 'frame.len', 'tcp.len', 'tcp.ack',\n",
        "                               'tcp.connection.fin', 'tcp.connection.rst', 'tcp.connection.sack',\n",
        "                               'tcp.connection.syn', 'tcp.flags.urg', 'tcp.hdr_len', 'tcp.pdu.size',\n",
        "                               'tcp.window_size_value', 'mqtt.clientid_len', 'mqtt.conack.val',\n",
        "                               'mqtt.conflag.passwd', 'mqtt.conflag.qos', 'mqtt.conflag.reserved',\n",
        "                               'mqtt.conflag.retain', 'mqtt.conflag.willflag', 'mqtt.dupflag',\n",
        "                               'mqtt.kalive', 'mqtt.len', 'mqtt.topic_len', 'mqtt.willmsg_len',\n",
        "                               'ip.proto', 'ip.ttl'], axis=1)\n",
        "    count_ICU_0, count_ICU_1 = dataset.label.value_counts()\n",
        "\n",
        "    df_ICU_0 = dataset[dataset['label'] == 0]\n",
        "    df_ICU_1 = dataset[dataset['label'] == 1]\n",
        "\n",
        "    df_ICU_0_under = df_ICU_0.sample(count_ICU_1)\n",
        "    df_ICU_under = pd.concat([df_ICU_0_under, df_ICU_1], axis = 0)\n",
        "    y = df_ICU_under.pop('label')\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((df_ICU_under.values, y.to_frame().values))\n",
        "    dataset = dataset.shuffle(4048, seed=SEED) #2048\n",
        "    if batch_size:\n",
        "        dataset = dataset.batch(batch_size)\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZaGwAccGRG7"
      },
      "outputs": [],
      "source": [
        "SEED=1337"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xV72Zd9JGNyw"
      },
      "outputs": [],
      "source": [
        "#########################################ECU#######################################################\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "train_data, val_data = [], []\n",
        "\n",
        "for client_data in [ICU1_shuffled, ICU2_shuffled, ICU3_shuffled, ICU4_shuffled, ICU5_shuffled,\n",
        "                    ICU6_shuffled, ICU7_shuffled, ICU8_shuffled, ICU9_shuffled, ICU10_shuffled,\n",
        "                    ICU11_shuffled, ICU12_shuffled, ICU13_shuffled, ICU14_shuffled, ICU15_shuffled]:\n",
        "\n",
        "\n",
        "    train_df, val_df = train_test_split(client_data, test_size=0.2, random_state=1337)\n",
        "\n",
        "    # Scaling (Standardization actually hurts performance)\n",
        "    encoder = LabelEncoder()\n",
        "    scaler = MinMaxScaler()\n",
        "    #scaler = StandardScaler()\n",
        "    train_features = scaler.fit_transform(train_df.drop(['label'], axis=1))\n",
        "    val_features = scaler.transform(val_df.drop(['label'], axis=1))\n",
        "\n",
        "    #encoder = LabelEncoder()\n",
        "    #y1 = encoder.fit_transform(y)\n",
        "    #Y= pd.get_dummies(y1).values\n",
        "    train_df[train_df.columns.difference(['label'])] = train_features\n",
        "    val_df[val_df.columns.difference(['label'])] = val_features\n",
        "\n",
        "    # TF Datasets\n",
        "    train_data.append(make_tf_dataset(train_df, batch_size=BATCH_SIZE)) #negative_ratio=2\n",
        "    val_data.append(make_tf_dataset(val_df, batch_size=16))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESvIpoanGZ7A"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.metrics import CategoricalAccuracy, Precision, Recall, BinaryAccuracy\n",
        "def input_spec():\n",
        "    return (\n",
        "        tf.TensorSpec([None, 11], tf.float64),\n",
        "        tf.TensorSpec([None, 1], tf.int64)\n",
        "    )\n",
        "\n",
        "def model_fn():\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.InputLayer(input_shape=(11,)),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dense(32, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid'),\n",
        "    ])\n",
        "\n",
        "    return tff.learning.from_keras_model(\n",
        "        model,\n",
        "        input_spec=input_spec(),\n",
        "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "        metrics=[tf.keras.metrics.BinaryAccuracy(), Precision(), Recall()]\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJ5yrJjlGfFm"
      },
      "outputs": [],
      "source": [
        "trainer = tff.learning.build_federated_averaging_process(\n",
        "    model_fn,\n",
        "    client_optimizer_fn=lambda: tf.keras.optimizers.Adam(learning_rate=0.01),\n",
        "    server_optimizer_fn=lambda: tf.keras.optimizers.Adam(learning_rate=0.05)#learning_rate=0.01\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_eDmZpCnGhrk"
      },
      "outputs": [],
      "source": [
        "str(trainer.initialize.type_signature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcjAopFVGkJH"
      },
      "outputs": [],
      "source": [
        "state = trainer.initialize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKatUyXQpBGy"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "start = time.time()\n",
        "start_time = time.time()\n",
        "end = time.time()\n",
        "diff=end-start\n",
        "starttest = time.time()\n",
        "endtest =time.time()\n",
        "difftest = endtest-starttest\n",
        "#state = trainer.initialize()\n",
        "train_hist = []\n",
        "for i in range(EPOCHS):\n",
        "    state, metrics = trainer.next(state, train_data)\n",
        "    train_hist.append(metrics)\n",
        "\n",
        "    print(f\"\\rRun {i+1}/{EPOCHS}\", end=\"\")\n",
        "endtest =time.time()\n",
        "#difftest = endtest-starttest\n",
        "print(\"\\nTraining time: \" + str(diff))\n",
        "print(\"Test time: \" + str(difftest))\n",
        "time_required = time.time() - start_time\n",
        "print('\\nTIME: {}seconds'.format(time_required))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vw1bX5zUp7ef"
      },
      "outputs": [],
      "source": [
        "train_hist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOSRMtBpp6Rj"
      },
      "outputs": [],
      "source": [
        "evaluator = tff.learning.build_federated_evaluation(model_fn)\n",
        "federated_metrics = evaluator(state.model, val_data)\n",
        "federated_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeq0LCYRIKWl"
      },
      "source": [
        "# ***Ten Clients***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o88ncTnSImux"
      },
      "outputs": [],
      "source": [
        "ICU1 = pd.read_csv('/content/drive/MyDrive/10-ICU-Env/ICU-Env1.csv')\n",
        "ICU1_shuffled = ICU1.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "ICU2 = pd.read_csv('/content/drive/MyDrive/10-ICU-Env/ICU-Env2.csv')\n",
        "ICU2_shuffled = ICU2.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "ICU3 = pd.read_csv('/content/drive/MyDrive/10-ICU-Env/ICU-Env3.csv')\n",
        "ICU3_shuffled = ICU3.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "ICU4 = pd.read_csv('/content/drive/MyDrive/10-ICU-Env/ICU-Env4.csv')\n",
        "ICU4_shuffled = ICU4.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "ICU5 = pd.read_csv('/content/drive/MyDrive/10-ICU-Env/ICU-Env5.csv')\n",
        "ICU5_shuffled = ICU5.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "ICU6 = pd.read_csv('/content/drive/MyDrive/10-ICU-Env/ICU-Env6.csv')\n",
        "ICU6_shuffled = ICU6.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "ICU7 = pd.read_csv('/content/drive/MyDrive/10-ICU-Env/ICU-Env7.csv')\n",
        "ICU7_shuffled = ICU7.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "ICU8 = pd.read_csv('/content/drive/MyDrive/10-ICU-Env/ICU-Env8.csv')\n",
        "ICU8_shuffled = ICU8.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "ICU9 = pd.read_csv('/content/drive/MyDrive/10-ICU-Env/ICU-Env9.csv')\n",
        "ICU9_shuffled = ICU9.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "ICU10 = pd.read_csv('/content/drive/MyDrive/10-ICU-Env/ICU-Env10.csv')\n",
        "ICU10_shuffled = ICU10.sample(frac=1, random_state = 13).reset_index(drop = True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftIClsB9s5lg"
      },
      "outputs": [],
      "source": [
        "ICU10_shuffled.label.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3TK9kwKJtu7"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 100\n",
        "#BATCH_SIZE = 512\n",
        "BATCH_SIZE = 1024 #ECU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2hAWT0DJylz"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def make_tf_dataset(dataframe1, batch_size=None):\n",
        "\n",
        "    dataset = dataframe1.drop(['Unnamed: 0', 'frame.time_relative', 'frame.len', 'tcp.len', 'tcp.ack',\n",
        "                               'tcp.connection.fin', 'tcp.connection.rst', 'tcp.connection.sack',\n",
        "                               'tcp.connection.syn', 'tcp.flags.urg', 'tcp.hdr_len', 'tcp.pdu.size',\n",
        "                               'tcp.window_size_value', 'mqtt.clientid_len', 'mqtt.conack.val',\n",
        "                               'mqtt.conflag.passwd', 'mqtt.conflag.qos', 'mqtt.conflag.reserved',\n",
        "                               'mqtt.conflag.retain', 'mqtt.conflag.willflag', 'mqtt.dupflag',\n",
        "                               'mqtt.kalive', 'mqtt.len', 'mqtt.topic_len', 'mqtt.willmsg_len',\n",
        "                               'ip.proto', 'ip.ttl'], axis=1)\n",
        "    count_ICU_0, count_ICU_1 = dataset.label.value_counts()\n",
        "\n",
        "    df_ICU_0 = dataset[dataset['label'] == 0]\n",
        "    df_ICU_1 = dataset[dataset['label'] == 1]\n",
        "\n",
        "    df_ICU_0_under = df_ICU_0.sample(count_ICU_1)\n",
        "    df_ICU_under = pd.concat([df_ICU_0_under, df_ICU_1], axis = 0)\n",
        "    y = df_ICU_under.pop('label')\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((df_ICU_under.values, y.to_frame().values))\n",
        "    dataset = dataset.shuffle(4048, seed=SEED) #2048\n",
        "    if batch_size:\n",
        "        dataset = dataset.batch(batch_size)\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DbZnvwW6J7e4"
      },
      "outputs": [],
      "source": [
        "SEED=1337"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6uO-DpOJ5Ka"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "train_data, val_data = [], []\n",
        "\n",
        "for client_data in [ICU1_shuffled, ICU2_shuffled, ICU3_shuffled, ICU4_shuffled, ICU5_shuffled,\n",
        "                    ICU6_shuffled, ICU7_shuffled, ICU8_shuffled, ICU9_shuffled, ICU10_shuffled]:\n",
        "\n",
        "\n",
        "    train_df, val_df = train_test_split(client_data, test_size=0.1, random_state=1337)\n",
        "\n",
        "    # Scaling (Standardization actually hurts performance)\n",
        "    encoder = LabelEncoder()\n",
        "    scaler = MinMaxScaler()\n",
        "    #scaler = StandardScaler()\n",
        "    train_features = scaler.fit_transform(train_df.drop(['label'], axis=1))\n",
        "    val_features = scaler.transform(val_df.drop(['label'], axis=1))\n",
        "\n",
        "    #encoder = LabelEncoder()\n",
        "    #y1 = encoder.fit_transform(y)\n",
        "    #Y= pd.get_dummies(y1).values\n",
        "    train_df[train_df.columns.difference(['label'])] = train_features\n",
        "    val_df[val_df.columns.difference(['label'])] = val_features\n",
        "\n",
        "    # TF Datasets\n",
        "    train_data.append(make_tf_dataset(train_df, batch_size=BATCH_SIZE)) #negative_ratio=2\n",
        "    val_data.append(make_tf_dataset(val_df, batch_size=16))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1kyDSLVKAfx"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.metrics import CategoricalAccuracy, Precision, Recall, BinaryAccuracy\n",
        "def input_spec():\n",
        "    return (\n",
        "        tf.TensorSpec([None, 11], tf.float64),\n",
        "        tf.TensorSpec([None, 1], tf.int64)\n",
        "    )\n",
        "\n",
        "def model_fn():\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.InputLayer(input_shape=(11,)),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dense(32, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid'),\n",
        "    ])\n",
        "\n",
        "    return tff.learning.from_keras_model(\n",
        "        model,\n",
        "        input_spec=input_spec(),\n",
        "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "        metrics=[tf.keras.metrics.BinaryAccuracy(), Precision(), Recall()]\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6D47kNHKFSe"
      },
      "outputs": [],
      "source": [
        "trainer = tff.learning.build_federated_averaging_process(\n",
        "    model_fn,\n",
        "    client_optimizer_fn=lambda: tf.keras.optimizers.Adam(learning_rate=0.01),\n",
        "    server_optimizer_fn=lambda: tf.keras.optimizers.Adam(learning_rate=0.05)#learning_rate=0.01\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXLjFllrKH6s"
      },
      "outputs": [],
      "source": [
        "str(trainer.initialize.type_signature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnituEwfKNNY"
      },
      "outputs": [],
      "source": [
        "state = trainer.initialize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVH4TgqIKQmX"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "start = time.time()\n",
        "start_time = time.time()\n",
        "end = time.time()\n",
        "diff=end-start\n",
        "starttest = time.time()\n",
        "endtest =time.time()\n",
        "difftest = endtest-starttest\n",
        "#state = trainer.initialize()\n",
        "train_hist = []\n",
        "for i in range(EPOCHS):\n",
        "    state, metrics = trainer.next(state, train_data)\n",
        "    train_hist.append(metrics)\n",
        "\n",
        "    print(f\"\\rRun {i+1}/{EPOCHS}\", end=\"\")\n",
        "endtest =time.time()\n",
        "#difftest = endtest-starttest\n",
        "print(\"\\nTraining time: \" + str(diff))\n",
        "print(\"Test time: \" + str(difftest))\n",
        "time_required = time.time() - start_time\n",
        "print('\\nTIME: {}seconds'.format(time_required))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCgrHcIku3t_"
      },
      "outputs": [],
      "source": [
        "train_hist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ch5WNi93KYY5"
      },
      "outputs": [],
      "source": [
        "evaluator = tff.learning.build_federated_evaluation(model_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b80kfORcKa42"
      },
      "outputs": [],
      "source": [
        "federated_metrics = evaluator(state.model, val_data)\n",
        "federated_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APai_bzBLrU6"
      },
      "source": [
        "# ***Five clients***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fblUh24hL1Nn"
      },
      "outputs": [],
      "source": [
        "ICU1 = pd.read_csv('/content/drive/MyDrive/5-ICU-Env/ICU-Env1.csv')\n",
        "ICU1_shuffled = ICU1.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "ICU2 = pd.read_csv('/content/drive/MyDrive/5-ICU-Env/ICU-Env2.csv')\n",
        "ICU2_shuffled = ICU2.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "ICU3 = pd.read_csv('/content/drive/MyDrive/5-ICU-Env/ICU-Env3.csv')\n",
        "ICU3_shuffled = ICU3.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "ICU4 = pd.read_csv('/content/drive/MyDrive/5-ICU-Env/ICU-Env4.csv')\n",
        "ICU4_shuffled = ICU4.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "ICU5 = pd.read_csv('/content/drive/MyDrive/5-ICU-Env/ICU-Env5.csv')\n",
        "ICU5_shuffled = ICU5.sample(frac=1, random_state = 13).reset_index(drop = True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3l9OkFUCMI-s"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 100\n",
        "#BATCH_SIZE = 512\n",
        "BATCH_SIZE = 1024 #ECU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Q0cad1Ow5Mh"
      },
      "outputs": [],
      "source": [
        "ICU5_shuffled.label.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcGfA1waMNhr"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def make_tf_dataset(dataframe1, batch_size=None):\n",
        "\n",
        "    dataset = dataframe1.drop(['Unnamed: 0', 'frame.time_relative', 'frame.len', 'tcp.len', 'tcp.ack',\n",
        "                               'tcp.connection.fin', 'tcp.connection.rst', 'tcp.connection.sack',\n",
        "                               'tcp.connection.syn', 'tcp.hdr_len', 'tcp.pdu.size',\n",
        "                               'tcp.window_size_value', 'mqtt.clientid_len', 'mqtt.dupflag',\n",
        "                               'mqtt.kalive', 'mqtt.len', 'mqtt.topic_len',\n",
        "                               'ip.proto', 'ip.ttl'], axis=1)\n",
        "    count_ICU_0, count_ICU_1 = dataset.label.value_counts()\n",
        "\n",
        "    df_ICU_0 = dataset[dataset['label'] == 0]\n",
        "    df_ICU_1 = dataset[dataset['label'] == 1]\n",
        "\n",
        "    df_ICU_0_under = df_ICU_0.sample(count_ICU_1)\n",
        "    df_ICU_under = pd.concat([df_ICU_0_under, df_ICU_1], axis = 0)\n",
        "    y = df_ICU_under.pop('label')\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((df_ICU_under.values, y.to_frame().values))\n",
        "    dataset = dataset.shuffle(4048, seed=SEED) #2048\n",
        "    if batch_size:\n",
        "        dataset = dataset.batch(batch_size)\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CK34XYS3MSdQ"
      },
      "outputs": [],
      "source": [
        "SEED=1337"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CAttzO6MUwi"
      },
      "outputs": [],
      "source": [
        "#########################################ECU#######################################################\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "train_data, val_data = [], []\n",
        "\n",
        "for client_data in [ICU1_shuffled, ICU2_shuffled, ICU3_shuffled, ICU4_shuffled, ICU5_shuffled]:\n",
        "\n",
        "\n",
        "    train_df, val_df = train_test_split(client_data, test_size=0.1, random_state=1337)\n",
        "\n",
        "    # Scaling (Standardization actually hurts performance)\n",
        "    encoder = LabelEncoder()\n",
        "    scaler = MinMaxScaler()\n",
        "    train_features = scaler.fit_transform(train_df.drop(['label'], axis=1))\n",
        "    val_features = scaler.transform(val_df.drop(['label'], axis=1))\n",
        "\n",
        "    #encoder = LabelEncoder()\n",
        "    #y1 = encoder.fit_transform(y)\n",
        "    #Y= pd.get_dummies(y1).values\n",
        "    train_df[train_df.columns.difference(['label'])] = train_features\n",
        "    val_df[val_df.columns.difference(['label'])] = val_features\n",
        "\n",
        "    # TF Datasets\n",
        "    train_data.append(make_tf_dataset(train_df,batch_size=BATCH_SIZE)) #negative_ratio=2\n",
        "    val_data.append(make_tf_dataset(val_df, batch_size=16))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-T6mxGNUMecS"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.metrics import CategoricalAccuracy, Precision, Recall, BinaryAccuracy\n",
        "def input_spec():\n",
        "    return (\n",
        "        tf.TensorSpec([None, 11], tf.float64),\n",
        "        tf.TensorSpec([None, 1], tf.int64)\n",
        "    )\n",
        "\n",
        "def model_fn():\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.InputLayer(input_shape=(11,)),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dense(32, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid'),\n",
        "    ])\n",
        "\n",
        "    return tff.learning.from_keras_model(\n",
        "        model,\n",
        "        input_spec=input_spec(),\n",
        "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "        metrics=[tf.keras.metrics.BinaryAccuracy(), Precision(), Recall()]\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHPqWdQlMh-w"
      },
      "outputs": [],
      "source": [
        "trainer = tff.learning.build_federated_averaging_process(\n",
        "    model_fn,\n",
        "    client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.01),\n",
        "    server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.05)#learning_rate=0.01\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AlSncZNbMkfR"
      },
      "outputs": [],
      "source": [
        "str(trainer.initialize.type_signature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3QyIJhUmMno5"
      },
      "outputs": [],
      "source": [
        "state = trainer.initialize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOSl5zjQMnW6"
      },
      "source": [
        "**test overfit**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcmJQTYiMi6b"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "start = time.time()\n",
        "start_time = time.time()\n",
        "end = time.time()\n",
        "diff=end-start\n",
        "starttest = time.time()\n",
        "endtest =time.time()\n",
        "difftest = endtest-starttest\n",
        "#state = trainer.initialize()\n",
        "train_hist = []\n",
        "for i in range(EPOCHS):\n",
        "    state, metrics = trainer.next(state, train_data)\n",
        "    train_hist.append(metrics)\n",
        "\n",
        "    print(f\"\\rRun {i+1}/{EPOCHS}\", end=\"\")\n",
        "endtest =time.time()\n",
        "#difftest = endtest-starttest\n",
        "print(\"\\nTraining time: \" + str(diff))\n",
        "print(\"Test time: \" + str(difftest))\n",
        "time_required = time.time() - start_time\n",
        "print('\\nTIME: {}seconds'.format(time_required))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WU9UXdcYMwDE"
      },
      "outputs": [],
      "source": [
        "train_hist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGt4Pnv6Mzb1"
      },
      "outputs": [],
      "source": [
        "evaluator = tff.learning.build_federated_evaluation(model_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VfcKfoMM32N"
      },
      "outputs": [],
      "source": [
        "federated_metrics = evaluator(state.model, val_data)\n",
        "federated_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXRKk63GM5hc"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uIQn81baMrRw"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "start = time.time()\n",
        "start_time = time.time()\n",
        "end = time.time()\n",
        "diff=end-start\n",
        "starttest = time.time()\n",
        "endtest =time.time()\n",
        "difftest = endtest-starttest\n",
        "#state = trainer.initialize()\n",
        "train_hist = []\n",
        "for i in range(EPOCHS):\n",
        "    state, metrics = trainer.next(state, train_data)\n",
        "    train_hist.append(metrics)\n",
        "\n",
        "    print(f\"\\rRun {i+1}/{EPOCHS}\", end=\"\")\n",
        "endtest =time.time()\n",
        "#difftest = endtest-starttest\n",
        "print(\"\\nTraining time: \" + str(diff))\n",
        "print(\"Test time: \" + str(difftest))\n",
        "time_required = time.time() - start_time\n",
        "print('\\nTIME: {}seconds'.format(time_required))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wE-vV9vUNypJ"
      },
      "outputs": [],
      "source": [
        "evaluator = tff.learning.build_federated_evaluation(model_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H41xnFYeN1aK"
      },
      "outputs": [],
      "source": [
        "federated_metrics = evaluator(state.model, val_data)\n",
        "federated_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ot7Bz8LhOFQx"
      },
      "outputs": [],
      "source": [
        "print(metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTNU9KQaOH3M"
      },
      "outputs": [],
      "source": [
        "train_hist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nzz1AGUV-Tj"
      },
      "source": [
        "# ***ICU-Patient***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1vcjIUmagwl"
      },
      "outputs": [],
      "source": [
        "df_ICU_PAI1 = pd.read_csv('/content/drive/MyDrive/ICU-Patient/clean_ICU-PAI.csv')\n",
        "df_ICU_PAI= df_ICU_PAI1.sample(frac=1, random_state = 13).reset_index(drop = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63Tz8O6fal__"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "for i, df in enumerate(np.array_split(df_ICU_PAI, 5)):\n",
        "    df.to_csv('/content/drive/MyDrive/5-ICU-PAI/'+f\"ICU-Pai{i+1}.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dh4lhEcjlX0D"
      },
      "source": [
        "# ***Fifteenclients***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7PiFN2EyslBP"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 100\n",
        "#BATCH_SIZE = 512\n",
        "BATCH_SIZE = 1024 #ECU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-12csj39iwUN"
      },
      "outputs": [],
      "source": [
        "ICU1 = pd.read_csv('/content/drive/MyDrive/10-ICU-Env1/ICU-Env1.csv')\n",
        "ICU1_shuffled = ICU1.sample(frac=1, random_state = 13).reset_index(drop = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13SxB2xJi3Hy"
      },
      "outputs": [],
      "source": [
        "ICU1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FUxf90Bi-3m"
      },
      "outputs": [],
      "source": [
        "ICU1_shuffled.label.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4eMZI-6ulp95"
      },
      "outputs": [],
      "source": [
        "ICU1 = pd.read_csv('/content/drive/MyDrive/15-ICU-PAI/ICU-Pai1.csv')\n",
        "ICU1_shuffled = ICU1.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "ICU2 = pd.read_csv('/content/drive/MyDrive/15-ICU-PAI/ICU-Pai2.csv')\n",
        "ICU2_shuffled = ICU2.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "ICU3 = pd.read_csv('/content/drive/MyDrive/15-ICU-PAI/ICU-Pai3.csv')\n",
        "ICU3_shuffled = ICU3.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "ICU4 = pd.read_csv('/content/drive/MyDrive/15-ICU-PAI/ICU-Pai4.csv')\n",
        "ICU4_shuffled = ICU4.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "ICU5 = pd.read_csv('/content/drive/MyDrive/15-ICU-PAI/ICU-Pai5.csv')\n",
        "ICU5_shuffled = ICU5.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "ICU6 = pd.read_csv('/content/drive/MyDrive/15-ICU-PAI/ICU-Pai6.csv')\n",
        "ICU6_shuffled = ICU6.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "ICU7 = pd.read_csv('/content/drive/MyDrive/15-ICU-PAI/ICU-Pai7.csv')\n",
        "ICU7_shuffled = ICU7.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "ICU8 = pd.read_csv('/content/drive/MyDrive/15-ICU-PAI/ICU-Pai8.csv')\n",
        "ICU8_shuffled = ICU8.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "ICU9 = pd.read_csv('/content/drive/MyDrive/15-ICU-PAI/ICU-Pai9.csv')\n",
        "ICU9_shuffled = ICU9.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "ICU10 = pd.read_csv('/content/drive/MyDrive/15-ICU-PAI/ICU-Pai10.csv')\n",
        "ICU10_shuffled = ICU10.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "ICU11 = pd.read_csv('/content/drive/MyDrive/15-ICU-PAI/ICU-Pai11.csv')\n",
        "ICU11_shuffled = ICU11.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "ICU12 = pd.read_csv('/content/drive/MyDrive/15-ICU-PAI/ICU-Pai12.csv')\n",
        "ICU12_shuffled = ICU12.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "ICU13 = pd.read_csv('/content/drive/MyDrive/15-ICU-PAI/ICU-Pai13.csv')\n",
        "ICU13_shuffled = ICU13.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "ICU14 = pd.read_csv('/content/drive/MyDrive/15-ICU-PAI/ICU-Pai14.csv')\n",
        "ICU14_shuffled = ICU14.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "ICU15 = pd.read_csv('/content/drive/MyDrive/15-ICU-PAI/ICU-Pai15.csv')\n",
        "ICU15_shuffled = ICU15.sample(frac=1, random_state = 13).reset_index(drop = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVQWF6C2ieyf"
      },
      "outputs": [],
      "source": [
        "ICU15_shuffled.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e83XPO2xnt9W"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def make_tf_dataset(dataframe, negative_ratio=None, batch_size=None):\n",
        "\n",
        "    dataset = dataframe.drop(['Unnamed: 0'], axis=1)#, 'SrcGap', 'DstGap', 'DIntPktAct','sMinPktSz', 'Trans'\n",
        "    # Class balancing\n",
        "    pos_df = dataset[dataset['label'] == 1]\n",
        "    neg_df = dataset[dataset['label'] == 0]\n",
        "    if negative_ratio:\n",
        "        neg_df = neg_df.iloc[random.sample(range(0, len(neg_df)), len(pos_df)*negative_ratio), :]\n",
        "    balanced_df = pd.concat([pos_df, neg_df], ignore_index=True, sort=False)\n",
        "\n",
        "    y = balanced_df.pop('label')\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((balanced_df.values, y.to_frame().values))\n",
        "    dataset = dataset.shuffle(4048, seed=SEED) #2048\n",
        "    if batch_size:\n",
        "        dataset = dataset.batch(batch_size)\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnz4I0NXse-Y"
      },
      "outputs": [],
      "source": [
        "SEED=1337"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4J3a94nHsFM0"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "train_data, val_data = [], []\n",
        "\n",
        "for client_data in [ICU1_shuffled, ICU2_shuffled, ICU3_shuffled, ICU4_shuffled, ICU5_shuffled,\n",
        "                    ICU6_shuffled, ICU7_shuffled, ICU8_shuffled, ICU9_shuffled, ICU10_shuffled,\n",
        "                    ICU11_shuffled, ICU12_shuffled, ICU13_shuffled, ICU14_shuffled, ICU15_shuffled]:\n",
        "\n",
        "\n",
        "    train_df, val_df = train_test_split(client_data, test_size=0.2, random_state=1337)\n",
        "\n",
        "    # Scaling (Standardization actually hurts performance)\n",
        "    encoder = LabelEncoder()\n",
        "    scaler = MinMaxScaler()\n",
        "    #scaler = StandardScaler()\n",
        "    train_features = scaler.fit_transform(train_df.drop(['label'], axis=1))\n",
        "    val_features = scaler.transform(val_df.drop(['label'], axis=1))\n",
        "\n",
        "    #encoder = LabelEncoder()\n",
        "    #y1 = encoder.fit_transform(y)\n",
        "    #Y= pd.get_dummies(y1).values\n",
        "    train_df[train_df.columns.difference(['label'])] = train_features\n",
        "    val_df[val_df.columns.difference(['label'])] = val_features\n",
        "\n",
        "    # TF Datasets\n",
        "    train_data.append(make_tf_dataset(train_df, batch_size=BATCH_SIZE)) #negative_ratio=2\n",
        "    val_data.append(make_tf_dataset(val_df, batch_size=16))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RbwHowgmsAm5"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.metrics import CategoricalAccuracy, Precision, Recall, BinaryAccuracy\n",
        "def input_spec():\n",
        "    return (\n",
        "        tf.TensorSpec([None, 37], tf.float64),\n",
        "        tf.TensorSpec([None, 1], tf.int64)\n",
        "    )\n",
        "\n",
        "def model_fn():\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.InputLayer(input_shape=(37,)),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dense(32, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid'),\n",
        "    ])\n",
        "\n",
        "    return tff.learning.from_keras_model(\n",
        "        model,\n",
        "        input_spec=input_spec(),\n",
        "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "        metrics=[tf.keras.metrics.BinaryAccuracy(), Precision(), Recall()]\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEoZIcfIr9xF"
      },
      "outputs": [],
      "source": [
        "trainer = tff.learning.build_federated_averaging_process(\n",
        "    model_fn,\n",
        "    client_optimizer_fn=lambda: tf.keras.optimizers.Adam(learning_rate=0.01),\n",
        "    server_optimizer_fn=lambda: tf.keras.optimizers.Adam(learning_rate=0.05)#learning_rate=0.01\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "puAWzLQpr7a5"
      },
      "outputs": [],
      "source": [
        "str(trainer.initialize.type_signature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lbr8Bz-Br30a"
      },
      "outputs": [],
      "source": [
        "state = trainer.initialize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Ci6i_rxZ6i3"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "start = time.time()\n",
        "start_time = time.time()\n",
        "end = time.time()\n",
        "diff=end-start\n",
        "starttest = time.time()\n",
        "endtest =time.time()\n",
        "difftest = endtest-starttest\n",
        "#state = trainer.initialize()\n",
        "train_hist = []\n",
        "for i in range(EPOCHS):\n",
        "    state, metrics = trainer.next(state, train_data)\n",
        "    train_hist.append(metrics)\n",
        "\n",
        "    print(f\"\\rRun {i+1}/{EPOCHS}\", end=\"\")\n",
        "endtest =time.time()\n",
        "#difftest = endtest-starttest\n",
        "print(\"\\nTraining time: \" + str(diff))\n",
        "print(\"Test time: \" + str(difftest))\n",
        "time_required = time.time() - start_time\n",
        "print('\\nTIME: {}seconds'.format(time_required))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJ_4qvZWaKYi"
      },
      "outputs": [],
      "source": [
        "train_hist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6P5fvXwqaJKt"
      },
      "outputs": [],
      "source": [
        "evaluator = tff.learning.build_federated_evaluation(model_fn)\n",
        "federated_metrics = evaluator(state.model, val_data)\n",
        "federated_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOoA6v85rxPP"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "start = time.time()\n",
        "start_time = time.time()\n",
        "end = time.time()\n",
        "diff=end-start\n",
        "starttest = time.time()\n",
        "endtest =time.time()\n",
        "difftest = endtest-starttest\n",
        "#state = trainer.initialize()\n",
        "train_hist = []\n",
        "for i in range(EPOCHS):\n",
        "    state, metrics = trainer.next(state, train_data)\n",
        "    train_hist.append(metrics)\n",
        "\n",
        "    print(f\"\\rRun {i+1}/{EPOCHS}\", end=\"\")\n",
        "endtest =time.time()\n",
        "#difftest = endtest-starttest\n",
        "print(\"\\nTraining time: \" + str(diff))\n",
        "print(\"Test time: \" + str(difftest))\n",
        "time_required = time.time() - start_time\n",
        "print('\\nTIME: {}seconds'.format(time_required))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPFx14zaruRr"
      },
      "outputs": [],
      "source": [
        "train_hist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0SbsDi-rprK"
      },
      "outputs": [],
      "source": [
        "evaluator = tff.learning.build_federated_evaluation(model_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3Cn5hqHrm9h"
      },
      "outputs": [],
      "source": [
        "federated_metrics = evaluator(state.model, val_data)\n",
        "federated_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7aQLoGumY-c"
      },
      "source": [
        "# ***Ten clients***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDmyrH-4mkwB"
      },
      "outputs": [],
      "source": [
        "ICU1 = pd.read_csv('/content/drive/MyDrive/10-ICU-PAI/ICU-Pai1.csv')\n",
        "ICU1_shuffled = ICU1.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "ICU2 = pd.read_csv('/content/drive/MyDrive/10-ICU-PAI/ICU-Pai2.csv')\n",
        "ICU2_shuffled = ICU2.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "ICU3 = pd.read_csv('/content/drive/MyDrive/10-ICU-PAI/ICU-Pai3.csv')\n",
        "ICU3_shuffled = ICU3.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "ICU4 = pd.read_csv('/content/drive/MyDrive/10-ICU-PAI/ICU-Pai4.csv')\n",
        "ICU4_shuffled = ICU4.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "ICU5 = pd.read_csv('/content/drive/MyDrive/10-ICU-PAI/ICU-Pai5.csv')\n",
        "ICU5_shuffled = ICU5.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "ICU6 = pd.read_csv('/content/drive/MyDrive/10-ICU-PAI/ICU-Pai6.csv')\n",
        "ICU6_shuffled = ICU6.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "ICU7 = pd.read_csv('/content/drive/MyDrive/10-ICU-PAI/ICU-Pai7.csv')\n",
        "ICU7_shuffled = ICU7.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "ICU8 = pd.read_csv('/content/drive/MyDrive/10-ICU-PAI/ICU-Pai8.csv')\n",
        "ICU8_shuffled = ICU8.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "ICU9 = pd.read_csv('/content/drive/MyDrive/10-ICU-PAI/ICU-Pai9.csv')\n",
        "ICU9_shuffled = ICU9.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "ICU10 = pd.read_csv('/content/drive/MyDrive/10-ICU-PAI/ICU-Pai10.csv')\n",
        "ICU10_shuffled = ICU10.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "471uBihTqfmZ"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 100\n",
        "#BATCH_SIZE = 512\n",
        "BATCH_SIZE = 1024 #ECU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LF1kK8YBnsnB"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def make_tf_dataset(dataframe, negative_ratio=None, batch_size=None):\n",
        "\n",
        "    dataset = dataframe.drop(['Unnamed: 0'], axis=1)#, 'SrcGap', 'DstGap', 'DIntPktAct','sMinPktSz', 'Trans'\n",
        "    # Class balancing\n",
        "    pos_df = dataset[dataset['label'] == 1]\n",
        "    neg_df = dataset[dataset['label'] == 0]\n",
        "    if negative_ratio:\n",
        "        neg_df = neg_df.iloc[random.sample(range(0, len(neg_df)), len(pos_df)*negative_ratio), :]\n",
        "    balanced_df = pd.concat([pos_df, neg_df], ignore_index=True, sort=False)\n",
        "\n",
        "    y = balanced_df.pop('label')\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((balanced_df.values, y.to_frame().values))\n",
        "    dataset = dataset.shuffle(4048, seed=SEED) #2048\n",
        "    if batch_size:\n",
        "        dataset = dataset.batch(batch_size)\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCj78YNFrBoW"
      },
      "outputs": [],
      "source": [
        "SEED=1337"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "em6Wl106qnTC"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "train_data, val_data = [], []\n",
        "\n",
        "for client_data in [ICU1_shuffled, ICU2_shuffled, ICU3_shuffled, ICU4_shuffled, ICU5_shuffled,\n",
        "                    ICU6_shuffled, ICU7_shuffled, ICU8_shuffled, ICU9_shuffled, ICU10_shuffled]:\n",
        "\n",
        "\n",
        "    train_df, val_df = train_test_split(client_data, test_size=0.2, random_state=1337)\n",
        "\n",
        "    # Scaling (Standardization actually hurts performance)\n",
        "    encoder = LabelEncoder()\n",
        "    scaler = StandardScaler()\n",
        "    train_features = scaler.fit_transform(train_df.drop(['label'], axis=1))\n",
        "    val_features = scaler.transform(val_df.drop(['label'], axis=1))\n",
        "\n",
        "    #encoder = LabelEncoder()\n",
        "    #y1 = encoder.fit_transform(y)\n",
        "    #Y= pd.get_dummies(y1).values\n",
        "    train_df[train_df.columns.difference(['label'])] = train_features\n",
        "    val_df[val_df.columns.difference(['label'])] = val_features\n",
        "\n",
        "    # TF Datasets\n",
        "    train_data.append(make_tf_dataset(train_df, batch_size=BATCH_SIZE)) #negative_ratio=2\n",
        "    val_data.append(make_tf_dataset(val_df, batch_size=16))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBGfCww7rIr2"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.metrics import CategoricalAccuracy, Precision, Recall, BinaryAccuracy\n",
        "def input_spec():\n",
        "    return (\n",
        "        tf.TensorSpec([None, 37], tf.float64),\n",
        "        tf.TensorSpec([None, 1], tf.int64)\n",
        "    )\n",
        "\n",
        "def model_fn():\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.InputLayer(input_shape=(37,)),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dense(32, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid'),\n",
        "    ])\n",
        "\n",
        "    return tff.learning.from_keras_model(\n",
        "        model,\n",
        "        input_spec=input_spec(),\n",
        "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "        metrics=[tf.keras.metrics.BinaryAccuracy(), Precision(), Recall()]\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88j7LcTUrNHe"
      },
      "outputs": [],
      "source": [
        "trainer = tff.learning.build_federated_averaging_process(\n",
        "    model_fn,\n",
        "    client_optimizer_fn=lambda: tf.keras.optimizers.Adam(learning_rate=0.01),\n",
        "    server_optimizer_fn=lambda: tf.keras.optimizers.Adam(learning_rate=0.05)#learning_rate=0.01\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlOT1AK5rPyD"
      },
      "outputs": [],
      "source": [
        "str(trainer.initialize.type_signature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zo__nNRdrTOp"
      },
      "outputs": [],
      "source": [
        "state = trainer.initialize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WUOdDSorXst"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "start = time.time()\n",
        "start_time = time.time()\n",
        "end = time.time()\n",
        "diff=end-start\n",
        "starttest = time.time()\n",
        "endtest =time.time()\n",
        "difftest = endtest-starttest\n",
        "#state = trainer.initialize()\n",
        "train_hist = []\n",
        "for i in range(EPOCHS):\n",
        "    state, metrics = trainer.next(state, train_data)\n",
        "    train_hist.append(metrics)\n",
        "\n",
        "    print(f\"\\rRun {i+1}/{EPOCHS}\", end=\"\")\n",
        "endtest =time.time()\n",
        "#difftest = endtest-starttest\n",
        "print(\"\\nTraining time: \" + str(diff))\n",
        "print(\"Test time: \" + str(difftest))\n",
        "time_required = time.time() - start_time\n",
        "print('\\nTIME: {}seconds'.format(time_required))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FhbUssKrcoY"
      },
      "outputs": [],
      "source": [
        "train_hist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJei4nTfrhOj"
      },
      "outputs": [],
      "source": [
        "evaluator = tff.learning.build_federated_evaluation(model_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xdz6_R6Brjcd"
      },
      "outputs": [],
      "source": [
        "federated_metrics = evaluator(state.model, val_data)\n",
        "federated_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nwd71fZ6mctm"
      },
      "source": [
        "# ***Five clients***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFhavzNimyye"
      },
      "outputs": [],
      "source": [
        "ICU1 = pd.read_csv('/content/drive/MyDrive/5-ICU-PAI/ICU-Pai1.csv')\n",
        "ICU1_shuffled = ICU1.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "ICU2 = pd.read_csv('/content/drive/MyDrive/5-ICU-PAI/ICU-Pai2.csv')\n",
        "ICU2_shuffled = ICU2.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "ICU3 = pd.read_csv('/content/drive/MyDrive/5-ICU-PAI/ICU-Pai3.csv')\n",
        "ICU3_shuffled = ICU3.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "ICU4 = pd.read_csv('/content/drive/MyDrive/5-ICU-PAI/ICU-Pai4.csv')\n",
        "ICU4_shuffled = ICU4.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n",
        "ICU5 = pd.read_csv('/content/drive/MyDrive/5-ICU-PAI/ICU-Pai5.csv')\n",
        "ICU5_shuffled = ICU5.sample(frac=1, random_state = 13).reset_index(drop = True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7JwdVkKqp81S"
      },
      "outputs": [],
      "source": [
        "ICU5_shuffled.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxV3bQEfnKAr"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 100\n",
        "BATCH_SIZE = 1024\n",
        "#BATCH_SIZE = 1024 #ECU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03AwtkQtnhAy"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def make_tf_dataset(dataframe, negative_ratio=None, batch_size=None):\n",
        "\n",
        "    dataset = dataframe.drop(['Unnamed: 0'], axis=1)#, 'SrcGap', 'DstGap', 'DIntPktAct','sMinPktSz', 'Trans'\n",
        "    # Class balancing\n",
        "    pos_df = dataset[dataset['label'] == 1]\n",
        "    neg_df = dataset[dataset['label'] == 0]\n",
        "    if negative_ratio:\n",
        "        neg_df = neg_df.iloc[random.sample(range(0, len(neg_df)), len(pos_df)*negative_ratio), :]\n",
        "    balanced_df = pd.concat([pos_df, neg_df], ignore_index=True, sort=False)\n",
        "\n",
        "    y = balanced_df.pop('label')\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((balanced_df.values, y.to_frame().values))\n",
        "    dataset = dataset.shuffle(4048, seed=SEED) #2048\n",
        "    if batch_size:\n",
        "        dataset = dataset.batch(batch_size)\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQ3wTL06oPQs"
      },
      "outputs": [],
      "source": [
        "SEED=1337"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWUHoAQdn0o_"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "train_data, val_data = [], []\n",
        "\n",
        "for client_data in [ICU1_shuffled, ICU2_shuffled, ICU3_shuffled, ICU4_shuffled, ICU5_shuffled]:\n",
        "\n",
        "\n",
        "    train_df, val_df = train_test_split(client_data, test_size=0.2, random_state=1337)\n",
        "\n",
        "    # Scaling (Standardization actually hurts performance)\n",
        "    encoder = LabelEncoder()\n",
        "    scaler = StandardScaler()\n",
        "    train_features = scaler.fit_transform(train_df.drop(['label'], axis=1))\n",
        "    val_features = scaler.transform(val_df.drop(['label'], axis=1))\n",
        "\n",
        "    #encoder = LabelEncoder()\n",
        "    #y1 = encoder.fit_transform(y)\n",
        "    #Y= pd.get_dummies(y1).values\n",
        "    train_df[train_df.columns.difference(['label'])] = train_features\n",
        "    val_df[val_df.columns.difference(['label'])] = val_features\n",
        "\n",
        "    # TF Datasets\n",
        "    train_data.append(make_tf_dataset(train_df, batch_size=BATCH_SIZE)) #negative_ratio=2\n",
        "    val_data.append(make_tf_dataset(val_df, batch_size=16))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCSsCsJpoU9Y"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.metrics import CategoricalAccuracy, Precision, Recall, BinaryAccuracy\n",
        "def input_spec():\n",
        "    return (\n",
        "        tf.TensorSpec([None, 37], tf.float64),\n",
        "        tf.TensorSpec([None, 1], tf.int64)\n",
        "    )\n",
        "\n",
        "def model_fn():\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.InputLayer(input_shape=(37,)),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dense(32, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid'),\n",
        "    ])\n",
        "\n",
        "    return tff.learning.from_keras_model(\n",
        "        model,\n",
        "        input_spec=input_spec(),\n",
        "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "        metrics=[tf.keras.metrics.BinaryAccuracy(), Precision(), Recall()]\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHT1bNygoaM1"
      },
      "outputs": [],
      "source": [
        "trainer = tff.learning.build_federated_averaging_process(\n",
        "    model_fn,\n",
        "    client_optimizer_fn=lambda: tf.keras.optimizers.Adam(learning_rate=0.01),\n",
        "    server_optimizer_fn=lambda: tf.keras.optimizers.Adam(learning_rate=0.05)#learning_rate=0.01\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYZqdC6doeDQ"
      },
      "outputs": [],
      "source": [
        "str(trainer.initialize.type_signature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C23CgUrbog19"
      },
      "outputs": [],
      "source": [
        "state = trainer.initialize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tr3HS925okjG"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "start = time.time()\n",
        "start_time = time.time()\n",
        "end = time.time()\n",
        "diff=end-start\n",
        "starttest = time.time()\n",
        "endtest =time.time()\n",
        "difftest = endtest-starttest\n",
        "#state = trainer.initialize()\n",
        "train_hist = []\n",
        "for i in range(EPOCHS):\n",
        "    state, metrics = trainer.next(state, train_data)\n",
        "    train_hist.append(metrics)\n",
        "\n",
        "    print(f\"\\rRun {i+1}/{EPOCHS}\", end=\"\")\n",
        "endtest =time.time()\n",
        "#difftest = endtest-starttest\n",
        "print(\"\\nTraining time: \" + str(diff))\n",
        "print(\"Test time: \" + str(difftest))\n",
        "time_required = time.time() - start_time\n",
        "print('\\nTIME: {}seconds'.format(time_required))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6V-NTXYotJF"
      },
      "outputs": [],
      "source": [
        "train_hist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFTt_PgVoxmS"
      },
      "outputs": [],
      "source": [
        "evaluator = tff.learning.build_federated_evaluation(model_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7NgPkp9o1sF"
      },
      "outputs": [],
      "source": [
        "federated_metrics = evaluator(state.model, val_data)\n",
        "federated_metrics"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "l_g5I_vOfCYq",
        "zU0981edis43",
        "LZ1pwE47f2hy",
        "nU9JNAqJvXW2",
        "aYwhaDYRzbAW",
        "RMdSjhqU2i1o",
        "V0Dp5MLX22xs",
        "6XPeTFsSA_a_",
        "7lx68vSwEqar",
        "8nzz1AGUV-Tj",
        "dh4lhEcjlX0D",
        "B7aQLoGumY-c",
        "Nwd71fZ6mctm"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
